Interface of the expert: One of the design goals of Canopy is to mimic [[the interface of speaking with an expert]], as opposed to [[other forms of knowledge representation]], because [[information that is presented in a human format is more likely to be assimilated]].

The interface of speaking with an expert: If we [[store information the way an expert does]], then we should be able to [[reproduce the interactions a person can have with an expert]].

Store information the way an expert does: [[Experts store information in small units]], and [[experts store information in densely connected units]].

Experts store information in small units: From that an expert can give a very brief explanation of a subject, or reuse the same small explanation in multiple larger explanations, we see that the units of information they are accessing are small.

Experts store information in densely connected units: From that an expert might mention a fact in many different contexts demonstrates that they have a dense network of connections between the different facts that they know.

Reproduce the interactions a person can have with an expert: A system that stores information like an expert does should be able to [[accept follow-up questions]], [[reuse explanations]] and [[mention interconnections]].

Accept follow-up questions: An expert might say a brief description of an idea, and the listener is able to inquire into any part of it, producing more statements, which the listener can pose more questions to. Canopy models this by presenting brief statements and having room for follow-up questions to be anticipated and addressed.

Reuse explanations: If a system stores explanations in small reusable units like an expert does, then that system should be able to produce larger customized explanations out of those reusable components, like an expert can.

Mention interconnections: If a system stores the dense interconnections between facts like an expert does, then that system should be able to mention those interconnections any time they are relevant, like an expert can.

Other forms of knowledge representation: Other systems attempt to [[model a set of facts logically]], even if [[people don't organize information logically]], or they [[model a set of facts using linear prose]], even if [[people don't organize information as linear prose]].

Model a set of facts logically: Many systems of knowledge representation represent facts as a set of logical connections between entities, and so "A is related to B" and "B is related to A" are represented by the same connection between A and B, and you can't have one without the other.

People don't organize information logically: People do not organize knowledge as a simple set of relations between entities. For example, [[humans represent facts and their inferences separately]], and [[there is hierarchy to human fact storage]].

Humans represent facts and their inferences separately: [[A person can know a fact but not its inferences]], and this makes sense because [[non-inferential fact storage is useful]].

A person can know a fact but not its inferences: It seems possible for a person, when they think about A, to remember that "A is related to B", but when they think about B, to not remember that "B is related to A," which seems to indicate that these facts are stored in the mind separately, as opposed to being stored as a single bidirectional connection between entities.

Non-inferential fact storage is useful: It can be helpful to store facts without their inferences. For example, I might want to [[record my intention to do a task]] without [[adding it to my list of things to do]], but I can't do that with logical storage because [[storing facts and inferences together destroys information about directionality]].

Record my intention to do a task: When I think about going to the store, I might want to remember that it is something I'd like to do, but only when I was thinking about going to the store already.

Adding it to my list of things to do: On the other hand, I might want to put "going to the store" on my list of "things to do," such that I will think about it any time I think about things I want to do, which might be much more frequently than I think about going to the store independently.

Storing facts and inferences together destroys information about directionality: If my intention to go to the store is stored as a simple bidirectional relationship between "things I want to do" and "going to the store," then I cannot label "going to the store" as "something I want to do" without adding it to my master list of "things I want to do," resulting in an overly long and disorganized to-do list.

Model a set of facts using linear prose: Some systems try to represent human knowledge as a series of articles on different subjects, with each article representing what a person might know about that subject.

People don't organize information as linear prose: Linear prose fails to capture many qualities of human knowledge, for example, that [[people store information as large collections of small units]] whereas [[linear prose stores knowledge as small collections of large units]].

People store information as large collections of small units: [[A person stores information about a subject as small units]], but [[a person can store a large number of units]].

A person stores information about a subject as small units: When you ask an expert for an explanation of a subject, they can give you a very brief explanation, and can then choose to expand on it, indicating that the units they actually store the information in are small.

A person can store a large number of units: Even the units of a person's knowledge are small, a person can string together immense structures of information build out of these small pieces.

Linear prose stores knowledge as small collections of large units: [[Linear prose stores information in large units]], but [[the total size of linear prose is limited]].

Linear prose stores information in large units: The typical article is much longer than anything a person would say from memory, indicating that prose is stored in much larger units than human memory is.

The total size of linear prose is limited: Even though the units of prose are large, the total amount of information that can be stored in a book or article is fixed, unlike human memory which has no obvious limit.

There is hierarchy to human fact storage: [[People segment relations by scale]], but [[scale information is lost in logical fact storage]].

People segment relations by scale: If a person wants to memorize the cities of a country, they might memorize a list of the top three or four, and then store the rest by region, eg "cities of the American Northeast." Thus, they would still be able to derive the full list, but they have organized the information by magnitude so that more important examples come to mind before less important ones.

Scale information is lost in logical fact storage: If the cities of a country are stored as simple bidirectional relationships between the the city and the country, then there is no simple way to label the city as belonging to the country without storing the city on the list of the country's cities, resulting in a long undifferentiated list of cities where less significant examples come to mind as frequently as more significant ones.

Information that is presented in a human format is more likely to be assimilated: There are many projects that allow a user to navigate a network of information, but their goal is not to produce _the_ output that an expert would. Experts producing explanation are performing a traversal of information stored in "the human knowledge format," whatever that is, and information produced from the human knowledge format is more likely to be directly assimilable by listeners _into_ the human knowledge format, and so by trying to preserve that format, Canopy presents information in the way most likely to be mentally reconstructed by readers into the desired understanding.

, in whatever format it is stored, and so a system that attempts to preserve the structure of expert output is more likely to produce content that listeners can reconstruct into a human format of knowledge.
