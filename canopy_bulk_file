[Canopy]

* Canopy JS:
Welcome! This is the documentation of the Canopy JS project. You can use Canopy to create interactive explanatory websites like this one.
There are various [[motivations for Canopy]].
These are instructions for [[how readers use Canopy]] and [[how writers use Canopy]], and this is an explanation of [[how Canopy works]].
Canopy has a [[codebase]] which you can find on the [project Github page](https://github.com/canopy-js/canopy-js).
(Try using the arrow keys!)


[Code]

* Codebase: The Canopy codebase is composed of a CLI and a front-end client.


[Data model]

* Canopy Data Model: Content in Canopy is composed of data in the form of [[topic|topics]] and [[subtopic|subtopics]].


* Global reference: A global reference is a link to a [[topic]], as opposed to a [[local reference]].


* Local reference: A local reference is a link to a [[subtopic]], as opposed to a [[global reference]].


* Subtopic: A subtopic is an entity that requires context before it can be explained, as opposed to a [[topic]]. A subtopic has a [[topic]]. Here's an [[example of a subtopic]]. Because a subtopic requires context, it is displayed at a [[subtopic path]].

Subtopic path: A subtopic exists within a certain [[topic]], which has a [[tree]], and a given subtopic paragraph exists at a path on that tree.

Example of a subtopic: An example of a subtopic is "Bob's nose." The topic of this subtopic is "Bob." "Bob's nose" can only be explained after explaining who "Bob" is.


* Subtopic paragraph:


* Topic: Topics are the fundamental organizing units of a Canopy project. A Canopy topic is a unit of explanation corresponding to some entity. A topic is an entity that can be explained without prior context, as opposed to a [[subtopic]]. A topic in Canopy has a [[topic paragraph]], and it has child paragraphs that form a [[tree]]. The data of a topic is stored in a [[topic file]].

Tree: A topic has a tree which is formed by the root [[topic paragraph]], which contains [[local reference|local references]], which connect it to [[subtopic paragraph|subtopic paragraphs]], which also contain local references, which connect it to further subtopic paragraphs, forming a tree.


* Topic file:


* Topic paragraph:


[Mechanics]

* How Canopy works:


[Mechanics/CLI]

* Canopy init: The `canopy init` command initializes the project. That entails creating a [[topics directory]], creating a [[default topic indicator file]], and creating the [[default topic file]] and a [default category].


[Mechanics/Project]

* Canopy project directory structure: ABC.


* Default Topic File: The default topic file is a topic file in the [[topics directory]]


* Default Topic Indicator File:


* Topics Directory: The topics directory is a directory found in the root level of a Canopy project. It contains subdirectories which represent [categories], and those subdirectories contain [topic files].


[Readers Interface]

* How readers use Canopy: How do readers use Canopy? The page begins with a paragraph explaining the selected [[topic]], and the user selects links to explore the content. You can [[navigate by mouse]], or [[keyboard shortcuts]].

Navigate by mouse: To navigate by mouse, click a link to inline its paragraph below the current one. If the link is a [[global reference]],

Keyboard shortcuts: ABC.


[Writers Interface]

* Adding content: Content is added to a Canopy project by creating files that follow the [[Canopy data model]].


* Deploying the project: ABC.


* How writers use Canopy: Writers use Canopy by [[creating a project]], [[adding content]], [building the project], [serving the project locally], and [[deploying the project]].

Creating a project: Creating a Canopy project involves [[creating a project directory]] and [[initializing the project]].

Creating a project directory: Creating the project directory is done by creating a directory, eg running `mkdir my_project` on a Unix-like system. There is nothing special about the type of directory.

Initializing the project: To initialize a project, enter the project directory (`cd` on a Unix system), then run `canopy init`. The [[canopy init]] command creates various files and folders that are necessary for your Canopy project.


[motivations]

* Motivations for Canopy: The motivations for Canopy include [[realism]], [[customization]], [[reusability]], [[clarity]], [[scale]], [[non-rival media]], [[fragmentation]], [[interconnection]], [[discoverability]], [[completeness]], [[hierarchy]], [[interface of the expert]], [[relative addressing]], [[idea addressing]], [[mergability]], [[memorability]], [[explicit prerequisites]], [[explicit state]], [[inline inclusion]], [[proportionality]], [[natural language classification]], [[gaps]], [[rich compositions]], [[prescriptive understandings]], [[holistic review]], [[pervasive concepts]], [[redundant indexing]], and [[explanatory invariants]].


[motivations/Clarity]

* Clarity: Clarity is the idea that [[explanations can contain extra information that assists the listener in reconstructing the expert's understanding]], and [[Canopy forces authors to include this information]].

Explanations can contain extra information that assists the listener in reconstructing the expert's understanding: [[Explanation has a nested structure]], but [[the connections between parts of an explanation can be unclear]], and so [[elaborate verbalization can help the listener parse the explanation]].

Explanation has a nested structure: Explanation is a description of something, and it is a description that uses terms which sometimes must themselves be explained, and sometimes those explanations include terms that need explanation, so at any given point, the explainer might be explaining a term in order to use it in the explanation of a term, and so on.

The connections between parts of an explanation can be unclear: It is sometimes unclear to the audience how the thing the author is currently speaking about relates to the original topic. For example, a story which is now being told might be intended to illustrate the meaning of a term which the speaker wants to use to describe the original topic, but the author might not have said so explicitly because it seemed obvious. If this remains unclear to the listeners, they will end up with a fragmented understanding, remembering the story itself but not connecting it to their understanding of the original topic.

Elaborate verbalization can help the listener parse the explanation: The author can help the listeners follow the flow of the explanation by adding redundant explicit explanation of how each step of the explanation relates to the previous one. The author might say "I want to explain a certain historical event, but first, in order to understand why it happened, I am going to explain a principle of economics. But, in order to understand this principle of economics, I need to tell you a story that illustrates it," and so on. Thus, every piece of the explanation is clearly related to the piece that came before it.

Canopy forces authors to include this information: In Canopy, every idea is expressed in a paragraph, and the only way to get from one paragraph to another is if the first paragraph mentions a concept that is explained by another. Thus, it is physically impossible for an author to leave it ambiguous why they are discussing a certain point, because by definition, the only way to access a point is by tracing the path to it from the original topic. Thus authors are forced to include this clarifying information even when it seems clear to them already, helping those readers for whom it wasn't clear.


[motivations/Completeness]

* Completeness: Completeness is the idea that once an author is able to [[scale|produce works at larger scales]], it will be possible to achieve completeness of analysis, such as [[recursive consideration of arguments]], and [[non-inclusion will be clearly intentional]].

Recursive consideration of arguments: Sometimes an author will give an argument for a view, and maybe also show a problem with the argument, but in theory, the tree of arguments and problems with the arguments, and problems with the problems, etc, can become very large, and it is difficult to achieve such coverage of even a small subject area.

Non-inclusion will be clearly intentional: At present, if an author doesn't include a given point or argument, they can claim it is because there isn't space to consider every view. With [[non-rival media]] however, when the author doesn't include a point, it will be more clearly a choice, which might motivate authors to present wider coverage.


[motivations/Customization]

* Customization: Customization is the idea that whereas [[a human explainer can customize explanations for a given listener]], a book, by contrast, is [[mostly the same experience for every reader]], and while [[linear text is efficient for homogeneous groups]], [[Canopy aims to provide the type of customization a human explainer can provide]].

A human explainer can customize explanations for a given listener: A human explainer can follow the listener's particular interests, adding more detail for someone advanced, or more background information for a beginner.

Mostly the same experience for every reader: A reader can flip around and read some chapters and not others, but fundamentally they are consuming long fixed blocks of linear content, unlike spoken explanation where the units are small, and the direction of the explanation can change based on feedback from the listener.

Linear text is efficient for homogeneous groups: Sometimes you have a group of people who want an explanation, and due to the circumstances, [[their needs are very similar]]. In these cases, [[it is more efficient to use linear prose than it is to use Canopy]].

Their needs are very similar: Two examples of groups of people with similar needs are [[standardized test takers]] and [[news readers]].

Standardized test takers: If students are taking a standardized test, and it is testing them on a subject area that is new to all of them, and the test requires the same level of knowledge for everbody, perhaps their background knowledge is identical, and their desired knowledge is identical, in which case they might not benefit from having customized explanations. (However, even in such a case it might be helpful to have [[customization in review]].)

Customization in review: When students are reviewing information, even if they all are aiming for the same level of final understanding, each particular student might benefit from reviewing different aspects of the content based on where they are having difficulties.

News readers: When an event occurs in the news and there isn't very much information available, most people are in the market for the same explanation, because that's all that is available. However, [[some news stories might benefit from customization]].

Some news stories might benefit from customization: [[Some news stories are complex]], and [[some news stories relate to background knowledge]], therefore some news stories do benefit from customization.

Some news stories are complex: When a news event involves an ongoing sequence of interrelated events, some readers might benefit a recap of the prior context, whereas others don't need it and would be annoyed by it.

Some news stories relate to background knowledge: If a news event has historical context or benefits from theoretical background knowledge, some readers may be interested to learn more about the related background knowledge, and some might just want the practical facts.

It is more efficient to use linear prose than it is to use Canopy: If a group of people all need the same explanation, writing it in linear text is more efficient than writing it in reusable blocks because [[the author can make better assumptions about the prior knowledge of the readers]], and [[the author can more easily capture learning requirements]].

The author can make better assumptions about the prior knowledge of the readers: [[An author of a book or article knows the context of the reader's session]], and [[a customized explanation does not]].

The author can more easily capture learning requirements: If the user is allowed to explore content freely, they might not learn everything they are responsible for knowing, whereas linear text can force them to go through a certain set of content.

An author of a book or article knows the context of the reader's session: An author of a book or article knows what the readers have read so far and what they are going to read, and so the author can reference things they know the reader has seen already, or avoid a certain subject for now because they know it will be explained later at a better time.

A customized explanation does not: Customized explanations are produced by combining reusable units that were not written with knowledge of what the reader has seen or will see, and so customized explanations might accidentally offer the reader information they already know from earlier in the session, or offer information before it is optimal for the reader to learn it.

Canopy aims to provide the type of customization a human explainer can provide: Canopy aims to provide customized explanations to each reader, allowing them to follow their specific interests and request additional information or clarification. ([[But isn't human explanation too complex to reproduce with digital tools?]])

But isn't human explanation too complex to reproduce with digital tools? One of the theses of Canopy is that [[explanation composition is complex]], but [[explanation assembly is not complex]], and [[Canopy only models explanation assembly]].

Explanation composition is complex: When a human composes a new explanation, they are drawing on the sum of their life experience and linguistic abilities, and this is an extremely complex process. In order for a computer to duplicate this task well, it would have to have similar breadth of knowledge to a human, which would be very difficult to accomplish.

Explanation assembly is not complex: The hope of Canopy is that if an author produces explanation in reusable blocks, the assembly of these blocks to fit the needs of the listener is a task that can be automated and done without requiring further input from the expert.

Canopy only models explanation assembly: The purpose of Canopy is to have the author express their knowledge in reusable units that the front-end client can combine into new explanations. Therefore, Canopy is not attempting to replace the author or the process of explanation composition, but only the latter stage of explanation assembly.


[motivations/Discoverability]

* Discoverability: Discoverability is the idea that [[it should be easy to find out what information exists in general]], and that [[it should be easy to find information related to a given point]]. Canopy attempts to [[make discovering the scope of available information easier]], and to [[make discovering related points easier]].

It should be easy to find out what information exists in general: Often it is very difficult even to just find out what information exists in a domain so that one can know what to request.

It should be easy to find information related to a given point: When someone sees a given piece of information, they should be made aware of other related information, which requires the media to represent the interconnections between the different pieces.

Make discovering the scope of available information easier: By [[decoupling detail from scope]], Canopy makes it easier to get a bird's-eye view of the corpus in order to inform one's requests for information.

Make discovering related points easier: By representing the [[interconnection]] between points, Canopy lets the user discover all the information related to what they're reading.

Decoupling detail from scope: Whereas linear prose will contain a mix of new points and details, in Canopy, details are displayed separately from new points, making it easier to see the scope of available content without getting bogged down in all the available details.


[motivations/Explanatory Invariants]

* Explanatory Invariants: Explanatory invariants is the idea that [[there are reoccuring types of relation between facts in a corpus]], and that [[it is sometimes desirable to capture every such relation]], and while [[free-form prose makes it difficult to capture explanatory invariants]], [[Canopy makes it easy to capture them]], [[without edging out existing analysis]].

There are reoccuring types of relation between facts in a corpus: You might have a corpus of historical facts and for every fact claim one could ask "what historical documents exist to corroborate this fact?" Or, you might have a description of the components of a system, and for each one you might ask "how does this component work?" Or for every idea stated in a discussion you can ask, "what is the historical origin of this idea?" – Thus, a corpus can be composed of reoccuring types of fact, and each type of fact might always have a certain related question.

It is sometimes desirable to capture every such relation: If we want to have [[completeness]] in capturing a corpus, it might be desirable to express the expectation that every fact of a certain type should have a relation of a certain type.

Free-form prose makes it difficult to capture explanatory invariants: When writing books and articles in free-form prose, different questions and forms of analysis are applied ad-hoc, and it can be difficult to see whether a certain question is asked uniformly in every place where it is relevant.

Canopy makes it easy to capture them: In Canopy, by modeling topics and all their relations in an abstract way before filling in the details, it is easier to see whether a certain question or relation is being expressed for every statement of a given type, and writers can make templates that require topics of a certain type to have different expected subcomponents, for example every historical event having a date or corroborating documents.

Without edging out existing analysis: If you take a book about different historical events and add a list of historical documents corroborating every point, then these added sources are making it harder for people to get the flow of the events themselves. Yet with Canopy, every point of type "historical claim" can have a subpoint "corroborating documentation" without these subpoints taking space away from the original analysis, because it is an example of [[non-rival media]].


[motivations/Explicit Prerequisites]

* Explicit Prerequisites: [[A linear work represents prerequisites implicitly via sequence]], [[making it difficult to consume small parts of a large work]], but [[Canopy represents scope explicitly via prerequisite enumeration]], [[making it easier to consume parts of a large work]].

A linear work represents prerequisites implicitly via sequence: In a book or lecture, the author assumes the listener knows everything that was said earlier in the session. Thus, the set prior knowledge that is assumed and might get referenced in a given paragraph is represented approximately by the set of all earlier paragraphs, even if not all prior paragraphs are actually necessary to understand a given point.

Making it difficult to consume small parts of a large work: If every paragraph could in theory reference ideas developed in any prior paragraph, then there isn't a good way to read just one paragraph of a book because you don't know which specific prior paragraphs it is building one.

Canopy represents scope explicitly via prerequisite enumeration: Canopy represents the prerequisites of each idea explicitly, and so you can in effect "read one paragraph from a book," and only those earlier paragraphs that are necessary to understand it. So, the "scope" of prior knowledge that the author is able to reference is represented explicitly via linked prerequisites, as opposed to implicitly via the sequence of the linear work.

Making it easier to consume parts of a large work: If the scope of each paragraph is defined explicitly, by labeling every prior idea that is mentioned explicitly, then it is very easy to pick a paragraph and read only those paragraphs that are necessary to understand it.


[motivations/Explicit State]

* Explicit state: [[Explanation has state]], but [[linear prose makes it unclear what the state of the explanation is]], whereas [[Canopy makes the state of the explanation visibly clear]].

Explanation has state: In the course of an explanation, we are at a particular point, there are prior points we are developing, and there are further specifications we could make, and a listener needs this information in order to understand what the explainer will say next.

Linear prose makes it unclear what the state of the explanation is: In linear prose, as I read a given paragraph, I can't easily see how we got to this point, or in what directions we can go, because our early context is scattered throughout the earlier portions, and the possible avenues of specification are scattered throughout the later portions.

Canopy makes the state of the explanation visibly clear: In Canopy, the screen doesn't show all prior content the reader has read in a given session, but only the path of paragraphs that explains how they got to the current subject, and links on the page describe all the directions the reader can go now given their position, either adding detail to the current paragraph, or regressing to an earlier idea and adding more detail to it.


[motivations/Fragmentation]

* Fragmentation: Fragmentation is the idea that [[when storage is coupled to presentation]], [[information will often be stored far from related information]], which [[harms discoverability]], and [[harms the development of mastery]].

When storage is coupled to presentation: If people only read from books and articles, then the way we store information will be by storing it as books and articles, and so the amount of information and interconnections we can store is constrained by the size and audience of the book, and so we are limited by the fact that the format of storage and the format of presentation have to be the same.

Information will often be stored far from related information: If understanding of a news story could benefit from an understanding of a historical fact, but not everyone would be interested in hearing about it, then the news story will be stored in a newspaper, and the historical fact will be stored in a textbook, and either the reader of the newspaper won't know about the historical fact, or at the very least there will be great friction in going from the newspaper to the historical fact.

Harms discoverability: If two pieces of information that are objectively related are stored in different places, then people who see one will not necessarily hear about the other, and at best it will be a missed opportunity for offering something of interest, and at worst the reader may form a completely incorrect understanding due to the missing fact.

Harms the development of mastery: If [[mastery requires interconnection]] and [[explanations are siloed]], then [[only experts will achieve any mastery]].

Mastery requires interconnection: Some portion of mastery over a subject matter is knowledge of the combinatorial interrelations between the pieces, such that if you see any part in any context, you can recognize the concept and connect it to the whole.

Explanations are siloed: If different explanations are stored in different books and articles, then they are in "silos," and someone reading one might not hear about information in another.

Only experts will achieve any mastery: Only experts will achieve mastery because [[an expert can overcome fragmented media]] but [[a casual reader retains media fragmentation]], and so [[non-experts cannot reach mastery over even small units]], [[creating large discrepancies in knowledge]].

An expert can overcome fragmented media: An expert might read all the works of a given domain many times, and so the fact that the information is stored in disconnected places is less of a problem because the expert will eventually see everything and connect it all in their mind, even if it isn't connected in the media itself.

A casual reader retains media fragmentation: A casual reader will have [[a fragmented understanding of the domain as a whole]], and [[a fragmented understanding of the portions of the domain they do learn]].

A fragmented understanding of the domain as a whole: A casual reader will not read all the works of a given domain, and so if information is fragmented into different disconnected works, the reader will come away with a fragmented understanding, only knowing about the points included in the particular work they read

A fragmented understanding of the portions of the domain they do learn: Even among the works a casual reader does read, they may fail to connect the information from the different works into a seamless network where every point is accessible by every other, producing a fragmented understanding where each point exists multiple times in different permutations, and a reference seen in one place doesn't end up pointing to the concept the author had in mind, producing vague and incomplete concepts.

Non-experts cannot reach mastery over even small units: Non-experts are prevented from reaching mastery over even small portions of a larger corpus, because if the information relevant to fully understanding the small portion might be distributed anywhere within the larger corpus, then only an expert who has the resources to assemble the entire corpus will be guaranteed to find it all.

Creating large discrepancies in knowledge: If only an expert who is able to learn everything about a domain can reach a nuanced understanding of any part of it, then the population will be divided into a few experts who know everything about a domain, and many people with no nuanced understanding of any part of it.


[motivations/Gaps]

* Gaps: "Gaps" is the idea that [[when transmitting a graph via smaller linearizations]], [[as the corpus gets larger, the chances of having incomplete information at any given point grows]], and [[even for information that is highly valued]], and [[Canopy helps by presenting information exhaustively]].

When transmitting a graph via smaller linearizations: If a person builds an understanding for example by reading many books and articles on the subject.

As the corpus gets larger, the chances of having incomplete information at any given point grows: If the domain is very large, there is a good chance that there are points that didn't make it into any of the particular books or articles read by the person.

Even for information that is highly valued: Even if someone is very interested in a subject, there is a good chance that there is basic information about something they would be very interested in hearing about that they happen to have never heard because it just never was relevant to include in any of the narrow books and articles they've happened to see on the topic.

Canopy helps by presenting information exhaustively: In Canopy, a topic's paragraph is an exhaustive list of all information in the graph about that topic, and so it is much harder for pieces of information to fall between the cracks, as opposed to someone who is constructing an understanding by reading various books and articles, none of which may be attempting to provide an exhaustive list.


[motivations/Hierarchy]

* Hierarchy: Hierarchy is the idea that [[in every complex domain, human understanding requires the creation of hierarchy]], and [[Canopy bakes hierarchy into the medium itself]].

In every complex domain, human understanding requires the creation of hierarchy: Because there are [[several limitations on the amount of information that can be dealt with at any given time]], [[humans deal with complexity by creating hierarchical models]], which [[enable them to manipulate complex information in small units]].

Several limitations on the amount of information that can be dealt with at any given time: Two such limitations are the fact that working memory can only store ideas composed of a certain number of "chunks", and that long-term memory can only directly associate a given fact or entity with a small number of other facts and entities.

Humans deal with complexity by creating hierarchical models: If a system has many interconnected parts, those who try to understand it might break it up into parts, or organize the interconnections between the pieces into categories so that different layers of the system can be studied one at a time.

Enable them to manipulate complex information in small units: By having ways of breaking a complex reality into simpler theoretical units, people are able to build models that are highly complex, without the author ever having to think about all the complexity at any single moment. By grouping the entities of the domain into larger hierarchical chunks, more information can be held in working memory at any given time, and by segmenting the connections different entities have by type, one can have good visibility into a single aspect of the network at each time.

Canopy bakes hierarchy into the medium itself: Canopy gives an author the ability to express a complex system out of a hierarchy of simpler parts, which makes it a natural tool for expressing the connections of a highly complex subject domain.


[motivations/Holistic Review]

* Holistic Review: Holistic review is the idea that rather than [[reviewing information in small pieces]], [[Canopy allows people to review information in large connected units]], which is possible because [[Canopy allows progressive occlusion of referenced prerequisites]].

Reviewing information in small pieces: When someone makes flash cards to learn an interconnected subject matter, they are reviewing pieces of that subject matter independently without also reviewing the connections at the same time.

Canopy allows people to review information in large connected units: By iterating repeatedly over a Canopy graph in different traversals, a reader is able to review not only the facts but also an organizing structure that includes the facts and relates them to one another.

Canopy allows progressive occlusion of referenced prerequisites: While [[a first time reader only can cover small portions of graph in each session]], [[over multiple reviews the size that can be reviewed in each session increases]], and because [[references reviewed in separate sessions can become fragmented]], [[larger review sessions produce a better resulting understanding]].

A first time reader only can cover small portions of graph in each session: When a person reads a Canopy paragraph for the first time, they might have to open many child paragraphs to understand all the different prerequisite concepts, so they end up not covering that much material because they need to burrow into so much clarification.

Over multiple reviews the size that can be reviewed in each session increases: As the reader reviews more and more, eventually they will remember the definitions of various terms and thus will not have to open as many child paragraphs, and so they can review larger and larger portions of the graph in one sitting.

References reviewed in separate sessions can become fragmented: If someone learns about New York on Sunday and hears that its capital is Albany, and then on Friday learns about New Jersey and hears that it is next to another state called New York, the person might fail to connect that the New York they heard about on Sunday is the same one they later heard about on Friday, and so they would fail to understand that New Jersey is next to a state whose capital is Albany, even though they've heard everything they need to understand it.

Larger review sessions produce a better resulting understanding: If a reader sees a description of something and then sees it referenced on the same day, there is a much better chance that this will produce a seamless, unfragmented understanding where new knowledge added to any point becomes accessible from every other.


[motivations/Idea addressing]

* Idea addressing: Idea addressing is that rather than having [[the unit of information]] be [[the category]] or [[the work]], [[we can address individual ideas]].

The unit of information: An information system is broken up into units that can be spoken about and recommended to other people individually. I can recommend a book or a chapter of a book, but it is hard to recommend a specific paragraph because it has no "name" in the system.

The category: In a traditional library, one unit of information is the category, ie the collection of works on a certain topic. The category has a name, so that people can communicate about it effectively.

The work: In a traditional library, one unit of information is the work, like a book or periodical. The work has a name so that people can communicate about it effectively.

We can address individual ideas: In Canopy, every idea gets a paragraph, and every idea gets a name, and from that name the idea gets a URL, and so we can speak about ideas and recommend explanations on a much finer-grain basis.


[motivations/Inline inclusion]

* Inline inclusion: Inline inclusion is the idea that [[natural language references are not semantically redirects]], yet [[in most hypertext systems references cause redirects]], so [[in Canopy references are inlined by default]].

Natural language references are not semantically redirects: When in conversation I mention something, my intention is usually not to suggest that we change the subject and discuss the idea for its own sake, but rather that we include a brief description of it here so that we can use it to clarify an idea we were already discussing.

In most hypertext systems references cause redirects: In most hypertext systems, content is composed of documents that have references to other documents, and selecting a reference causes the reader to redirect to another document, "changing the subject" to a discussion of the new idea in its own right, rather than as a means to understand the previous subject.

In Canopy references are inlined by default: When selecting a link in Canopy, by default the paragraph associated with the selected link as added to the current page under the paragraph of the link. This allows the reader to retain their place in the earlier explanation, and peruse an explanation of the new idea only insofar as is necessary to continue reading the earlier paragraph. This user experience is more similar to the natural experience of listening to conversation, and so there is a better chance that the reader will be able to assimilate the given information correctly.


[motivations/Interconnection]

* Interconnection: Interconnection is the idea that [[the connections between different facts are valuable data]], but that [[linear content limits the representation of interconnections]], whereas [[Canopy decouples information storage from presentation]], and so [[Canopy increases the number of connections that can be stored]], making it more similar to the [[interface of the expert]].

The connections between different facts are valuable data: Even if a person knows many facts, if they lack an understanding of the connection between them, they will miss implications from one to another, and new information might not update all relevant knowledge.

Linear content limits the representation of interconnections: When information is stored in linear books and articles, the author cannot share every relevant interconnection because [[audience interest is limited]] and [[the shared prior knowledge is small]].

Audience interest is limited: The audience might not be interested in hearing every connection between every idea, and so the author won't include them even for those readers who do.

The shared prior knowledge is small: The author can only include interconnections between facts that the readers know about, which limits the author to interconnections between facts mentioned in the given book or article, but because [[there are constraints on the size of books and articles]], this ends up meaning that [[there is no work that stores enough facts to also include all interconnections]].

There are constraints on the size of books and articles: Practically speaking a book or article can only become so long and retain reader interest.

There is no work that stores enough facts to also include all interconnections: No work has all the given points, and no work can connect points that it doesn't include, and so no work exists that can include all the interconnections of a domain, even if there would be reader interest.

Canopy decouples information storage from presentation: With Canopy, what the author writes and what the reader reads doesn't have to be the same thing.

Canopy increases the number of connections that can be stored: The author can store all the points and interconnections they know about, and not every reader has to read every one.


[motivations/Interface of the expert]

* Interface of the expert: One of the design goals of Canopy is to mimic [[the interface of speaking with an expert]], as opposed to [[other forms of knowledge representation]], because [[information that is presented in a human format is more likely to be assimilated]].

The interface of speaking with an expert: If we [[store information the way an expert does]], then we should be able to [[reproduce the interactions a person can have with an expert]].

Store information the way an expert does: [[Experts store information in small units]], and [[experts store information in densely connected units]].

Experts store information in small units: From that an expert can give a very brief explanation of a subject, or reuse the same small explanation in multiple larger explanations, we see that the units of information they are accessing are small.

Experts store information in densely connected units: From that an expert might mention a fact in many different contexts demonstrates that they have a dense network of connections between the different facts that they know.

Reproduce the interactions a person can have with an expert: A system that stores information like an expert does should be able to [[accept follow-up questions]], [[reuse explanations]] and [[mention interconnections]].

Accept follow-up questions: An expert might say a brief description of an idea, and the listener is able to inquire into any part of it, producing more statements, which the listener can pose more questions to. Canopy models this by presenting brief statements and having room for follow-up questions to be anticipated and addressed.

Reuse explanations: If a system stores explanations in small reusable units like an expert does, then that system should be able to produce larger customized explanations out of those reusable components, like an expert can.

Mention interconnections: If a system stores the dense interconnections between facts like an expert does, then that system should be able to mention those interconnections any time they are relevant, like an expert can.

Other forms of knowledge representation: Other systems attempt to [[model a set of facts logically]], even if [[people don't organize information logically]], or they [[model a set of facts using linear prose]], even if [[people don't organize information as linear prose]].

Model a set of facts logically: Many systems of knowledge representation represent facts as a set of logical connections between entities, and so "A is related to B" and "B is related to A" are represented by the same connection between A and B, and you can't have one without the other.

People don't organize information logically: People do not organize knowledge as a simple set of relations between entities. For example, [[humans represent facts and their inferences separately]], and [[there is hierarchy to human fact storage]].

Humans represent facts and their inferences separately: [[A person can know a fact but not its inferences]], and this makes sense because [[non-inferential fact storage is useful]].

A person can know a fact but not its inferences: It seems possible for a person, when they think about A, to remember that "A is related to B", but when they think about B, to not remember that "B is related to A," which seems to indicate that these facts are stored in the mind separately, as opposed to being stored as a single bidirectional connection between entities.

Non-inferential fact storage is useful: It can be helpful to store facts without their inferences. For example, I might want to [[record my intention to do a task]] without [[adding it to my list of things to do]], but I can't do that with logical storage because [[storing facts and inferences together destroys information about directionality]].

Record my intention to do a task: When I think about going to the store, I might want to remember that it is something I'd like to do, but only when I was thinking about going to the store already.

Adding it to my list of things to do: On the other hand, I might want to put "going to the store" on my list of "things to do," such that I will think about it any time I think about things I want to do, which might be much more frequently than I think about going to the store independently.

Storing facts and inferences together destroys information about directionality: If my intention to go to the store is stored as a simple bidirectional relationship between "things I want to do" and "going to the store," then I cannot label "going to the store" as "something I want to do" without adding it to my master list of "things I want to do," resulting in an overly long and disorganized to-do list.

Model a set of facts using linear prose: Some systems try to represent human knowledge as a series of articles on different subjects, with each article representing what a person might know about that subject.

People don't organize information as linear prose: Linear prose fails to capture many qualities of human knowledge, for example, that [[people store information as large collections of small units]] whereas [[linear prose stores knowledge as small collections of large units]].

People store information as large collections of small units: [[A person stores information about a subject as small units]], but [[a person can store a large number of units]].

A person stores information about a subject as small units: When you ask an expert for an explanation of a subject, they can give you a very brief explanation, and can then choose to expand on it, indicating that the units they actually store the information in are small.

A person can store a large number of units: Even the units of a person's knowledge are small, a person can string together immense structures of information build out of these small pieces.

Linear prose stores knowledge as small collections of large units: [[Linear prose stores information in large units]], but [[the total size of linear prose is limited]].

Linear prose stores information in large units: The typical article is much longer than anything a person would say from memory, indicating that prose is stored in much larger units than human memory is.

The total size of linear prose is limited: Even though the units of prose are large, the total amount of information that can be stored in a book or article is fixed, unlike human memory which has no obvious limit.

There is hierarchy to human fact storage: [[People segment relations by scale]], but [[scale information is lost in logical fact storage]].

People segment relations by scale: If a person wants to memorize the cities of a country, they might memorize a list of the top three or four, and then store the rest by region, eg "cities of the American Northeast." Thus, they would still be able to derive the full list, but they have organized the information by magnitude so that more important examples come to mind before less important ones.

Scale information is lost in logical fact storage: If the cities of a country are stored as simple bidirectional relationships between the the city and the country, then there is no simple way to label the city as belonging to the country without storing the city on the list of the country's cities, resulting in a long undifferentiated list of cities where less significant examples come to mind as frequently as more significant ones.

Information that is presented in a human format is more likely to be assimilated: There are many projects that allow a user to navigate a network of information, but their goal is not to produce _the_ output that an expert would. Experts producing explanation are performing a traversal of information stored in "the human knowledge format," whatever that is, and information produced from the human knowledge format is more likely to be directly assimilable by listeners _into_ the human knowledge format, and so by trying to preserve that format, Canopy presents information in the way most likely to be mentally reconstructed by readers into the desired understanding.

, in whatever format it is stored, and so a system that attempts to preserve the structure of expert output is more likely to produce content that listeners can reconstruct into a human format of knowledge.


[motivations/Memorability]

* Memorability: [[Memory entails associating a cue with a response]] which is achieved by [[seeing the cue alone before the response]] but [[traditional prose presents information as a single unit]], so [[Canopy presents information in stages]], and [[this will make content easier to remember]].

Memory entails associating a cue with a response: It seems that the major systems of memory create associations so that one particular thought brings to mind another, and so when we think about teaching students a certain corpus, we have to be clear about what specific cues we want them to recognize, and what specific responses we want them to think of when they see those cues.

Seeing the cue alone before the response: In order to make someone remember to buy milk when they pass the grocery store, they should think about "passing the grocery store" by itself, and then think about getting milk, so that when they are in the real world and are passing the grocery store, the idea of passing the store by itself brings the idea of getting milk to mind. If they merely review the idea of "getting milk when passing the grocery store" as one big unit, then they are associating the task in memory with itself, and so the situation of "passing the store" by itself might not bring it to mind, defeating the purpose of memorization.

Traditional prose presents information as a single unit: When I learn information about New York, I want that information to be the "response" that comes to mind when I think about the "cue" of New York. Prose, however, presents me the cue and the response all as one unit, and so I am not practicing retrieval of the content by topic name, but rather, I am practicing retrieval of the content by content, which isn't helpful.

Canopy presents information in stages: Canopy presents a brief paragraph that mentions a few entities by name, and only once the user interacts with a topic name does the user see more information, so the "cue" of the topic name is seen before the "response" of the topic's paragraph.

This will make content easier to remember: In Canopy, the reader sees the "cue" of the link before the "response" of the child paragraph, and so the reader is being trained to expect the paragraph when they see the topic name alone, creating a vast network of cues that trigger responses which contain in them multiple further cues, producing mental structures that will enable the reader to reliably retrieve later the new information they are receiving, preventing "orphans" where information is learned but without creating a cue by which to access it later.


[motivations/Mergability]

* Mergability: Mergability is the idea that whereas [[linear works address content approximately]], [[Canopy addresses ideas individually]], [[it is easier to merge multiple explanations]], [[making possible new types of composite works]].

Linear works address content approximately: In a book, ideas are organized by chapter and page number, and so the system of addressing isn't very precise.

Canopy addresses ideas individually: Every idea in Canopy has a unique address, and it exists at a point in a larger series of topics.

It is easier to merge multiple explanations: If two books speak about the same subjects, it is not very easy to "put them into conversation" and line up the bits that speak about the same things. With Canopy however, because the content is already organized in small labeled units, it might be much easier to line up one work with another and compare what they both say on a given topic.

Making possible new types of composite works: By making it easy to "merge" different works together, it might become possible to produce new types of work that cull all the points of a set of works by topic and present them in dialog, capturing every point made in the original works, but without following the exact train of thought of each original author, allowing readers to take a complex topic, break it into smaller questions, and see the range of perspectives that exist on each one. Thus, a group of people could produce a "balanced" presentation of views on all the points of a topic area even if they do not agree, so long as they agree on what question each view is coming to answer.


[motivations/Natural language classification]

* Natural language classification: Natural language classification is the idea that [[people use natural language as a classification scheme in conversation]], and [[Canopy follows the natural classification scheme]], related to the idea of [[relative addressing]].

People use natural language as a classification scheme in conversation: When someone says "there is a country called France and there are certain exports that it produces economically," the speaker has specified a category in which further information can be put. Similarly, when requesting information, a person might give a brief explanation and then pose a question about something it mentions. Thus, the introductory sentences themselves are functioning as a classification scheme for storing and requesting information.

Canopy follows the natural classification scheme: By having the reader navigate by reading paragraphs and selecting certain portions of them, the user is using the natural language content as the form of navigation, similar to how they would request information in a conversation.


[motivations/Non-rival media]

* Non-rival media: Non-rival media is the idea that [[linear mediums are rival]], whereas [[non-linear mediums like Canopy are non-rival]], and [[non-rival mediums have various benefits]].

Linear mediums are rival: In books and articles, including one point comes at the expense of space for another, and thus they are "rival," because each point crowds out some other point.

Non-linear mediums like Canopy are non-rival: [[Non-linear mediums are non-rival for space]], although [[they are rival for attention]].

Non-linear mediums are non-rival for space: Non-linear media can grow happily to any size, and so no point is taking space from any other point, because every category of point has a its own special location in the project.

They are rival for attention: Because Canopy presents information in a hierarchical format, even if there is unlimited space for points in general, there is limited space for a point to get "top billing" in a high-order paragraph. So in that sense even Canopy isn't entirely "non-rival."

Non-rival mediums have various benefits: When different points don't take space from one another, then authors can adopt an attitude of default inclusion, making space for any point any reader might want to see, even if most readers aren't interested, (thus creating works of greater [[scale]] and [[customization]].)


[motivations/Pervasive Concepts]

* Pervasive Concepts: Pervasive concepts is the idea that [[within a domain, there are certain pervasive concepts]], and [[it can be difficult for the reader to keep track of all the situations in which they've seen a pervasive concept]], and [[keeping track of different examples is be necessary to produce an abstract understanding of a pervasive concept]], and so [[Canopy helps by allowing you to link to pervasive concepts as you work through the material of a course]].

Within a domain, there are certain pervasive concepts: In programming for example, you have the concept of modularity, that large systems can be composed of smaller units that hide their internal structure in order to make things more understandable.

It can be difficult for the reader to keep track of all the situations in which they've seen a pervasive concept: Throughout a computer science education, a student might have seen a concept like modularity crop up in hundreds of situations, and if they were a more casual reader, they might have forgotten some of them.

Keeping track of different examples is be necessary to produce an abstract understanding of a pervasive concept: In order to have an abstract and flexible concept of modularity, it might be necessary to remember many different examples, and hold in mind at the same time the way in which each example has unique details but still expresses the concept abstractly.

Canopy helps by allowing you to link to pervasive concepts as you work through the material of a course: By using [[redundant indexing]], all the units of a curriculum can be accessible by topic, but also accessible by theme, and so a reader can review prior material specifically in so far as it illustrates a pervasive concept like modularity.


[motivations/Prescriptive Understandings]

* Prescriptive Understandings: Prescriptive understandings is the idea that [[experts have vast organizational structures]], but that [[experts omit portions of their mental representation when speaking]], and [[this prevents listeners from building the proper understanding]] but [[Canopy requires the author to enumerate the desired understanding]].

Experts have vast organizational structures: Experts don't just memorize a list of facts, they have many heuristic subcategories that are a function of their way of grouping things.

Experts omit portions of their mental representation when speaking: Sometimes an expert's private groupings are extremely useful practically to have, yet aspects of them may be controversial or oversimple, and so the expert might be motivated to omit them and just present the facts in sequence without offering any helpful mnemonics or groupings.

This prevents listeners from building the proper understanding: The expert is only able to navigate their knowledge using these helpful groupings, and so by removing them from the explanation, the expert is failing to fully enumerate the mental structure that the listener should produce from the data.

Canopy requires the author to enumerate the desired understanding: Canopy requires information be presented hierarchically, and so it isn't possible to give undifferentiated lists, and so authors will be forced to provide the reader with helpful intermediary groupings with which to retrieve the information in question.


[motivations/Proportionality]

* Proportionality: Proportionality is the idea that because [[Canopy organizes information hierarchically]], if [[one reads top to bottom]], [[one will end up with a proportional understanding]], [[unlike reading a book or article]].

Canopy organizes information hierarchically: Canopy takes a domain and lists the important topics in it, and for each topic it lists the major ideas, and for the major ideas it lists the details, etc.

One reads top to bottom: If one reads about all the big ideas before burrowing into details.

One will end up with a proportional understanding: Whether a person has one hour to read about a subject or one year, if they read the hierarchy top-down, then they will end up with the most accurate picture of the subject matter possible given that time constraint, because they are always reading about the big ideas before going into any details.

Unlike reading a book or article: When reading a book or article, if one only has time for a small portion of it, it is very hard to allocate that time effectively to find the subset of the work that paints the most accurate picture given the time constraint. One is likely to end up with certain concepts in great detail, while not knowing anything about certain other ones.


[motivations/Realism]

* Realism: Realism is the idea that [[human explanation really is the serialization of a graph]], and so [[any representation of explanation besides a graph will be distortive]], which is why [[Canopy represents the underlying explanation graph]], so that [[we preserve all possible explanations that can be produced from the original data]].

Human explanation really is the serialization of a graph: [[Humans explain things in serial]], so [[you might think that explanation is inherently serial]], but actually [[there are many valid serializations of a knowledge set]], and so we see that [[serializations are just one of many presentations of an underlying data structure]], and [[it makes sense to think that the underlying data structure is a graph]].

Humans explain things in serial: A book or lecture is inherently a list of statements, one after another, in a straight line. It has to be this way because speakers can only say one thing at a time and listeners can only listen to one thing at a time.

You might think that explanation is inherently serial: Because all of the explanations we see are linear, one thing after another, you might think that explanations are inherently linear, and that all an explanation is is a list of sentences.

There are many valid serializations of a knowledge set: We see that an expert can express the same knowledge in multiple traversals, for example when an expert writes multiple books and articles on the same topic but in different orderings and levels of detail.

Serializations are just one of many presentations of an underlying data structure: Because we see the same knowledge can be used to produce many different explanations, we see that linear explanations are not a true representation of the underling data, but are rather ephemeral permutations being generated on-the-fly from a more fundamental structure.

It makes sense to think that the underlying data structure is a graph: A graph is composed of a series of entities and connections, and so if knowledge were stored as a graph, it would make sense how you could make many different explanations from the same original data, because with a single graph you can explore the nodes via their connections in many different orderings.

Any representation of explanation besides a graph will be distortive: [[There is no good way to represent a large graph as a list]]: [[representing the graph with a breadth-first search will be distortive]], and [[representing the graph as a depth-first search will be distortive]].

There is no good way to represent a large graph as a list: Lets say an expert wants to write down everything they know into one or several books. No matter how they do it, inevitably, pieces of information that should be adjacent will end up very far apart in the resulting output because there is no way to put every fact nearby every related fact in a linear sequence.

Representing the graph with a breadth-first search will be distortive: If we [[represent the graph as a breadth-first search]], then [[it is easy to get a general picture]], but [[it will be hard for readers to get all the details of a subject]].

Represent the graph as a breadth-first search: Breadth-first search is a way of converting a graph into a linear text like a book. An example would be if we wrote a paragraph about New York that mentioned the economy, the geography, and the politics, followed by one paragraph on the economy, one paragraph on the geography, and one paragraph on the politics, and we kept expanding on every point equally before burrowing into depth on any one thing. This would be a "breadth-first" search, because we are going "broad" before we go "deep" on any one thing.

It is easy to get a general picture: If the reader wanted to get a general picture, they would just start at the beginning of the book and read onward, and they would get a perfectly broad depiction for however much they wanted to read because the explanation would add every major detail of every aspect before adding any minor details of any aspect.

It will be hard for readers to get all the details of a subject: If a book goes "broad" before going "deep" on any one point, it would be hard for a reader to find all the information about New York's economy in all its detail because these paragraphs would scattered around the explanation in different places.

Representing the graph as a depth-first search will be distortive: If we [[represent the graph as a depth-first search]], then [[it is easy to find all the information on a single subject]], but [[it will be hard for readers to get a general picture]].

Represent the graph as a depth-first search: Depth-first search is a way of converting a graph into a linear text like a book. An example would be if we wrote a paragraph about New York that mentioned the economy, the geography, and the politics, followed by 100 paragraphs that discussed the economy in every detail. This would be called a "depth-first" search because we are going "deep" before we are going "broad," hearing 100 things about the economy of New York before we hear even a single thing about the geography.

It is easy to find all the information on a single subject: If a reader wanted to find everything to do with the economy, they would just read the first third of the book front-to-back.

It will be hard for readers to get a general picture: If we organize data as a "depth-first" list, then it is hard for a reader to get a semi-detailed picture of New York's various aspects because all the information is segmented by topic: the basic information about economy is going to be in the first third of the book, organized with the economic information, the basic information about the geography is going to be in the second third, organized with the geographic information, and the basic information about politics is going to be in the last third, organized with the politics information, and so to get a general picture the reader would have to skim and jump around a lot.

Canopy represents the underlying explanation graph: Rather than have an author produce a series of books and articles, capturing their knowledge graph only approximately, Canopy has the author capture the underlying graph itself, offering tools to make this easier for them to do.

We preserve all possible explanations that can be produced from the original data: [[The author in person can produce many linearizations]], but [[capturing their knowledge as a sequence destroys this original flexibility]], however, [[if the author captures their knowledge with a graph]], [[we preserve the original multiplicity of possible explanations]].

The author in person can produce many linearizations: An expert in-person can produce many different linearizations of their knowledge for different people.

Capturing their knowledge as a sequence destroys this original flexibility: If the expert captures their knowledge as one big book, then we went from having the expert, who could produce infinitely many linearizations of their knowledge, to having a book, which is only one linearization, destroying all of the other potential explanations the expert could have produced.

If the author captures their knowledge with a graph: If the expert uses a tool like Canopy to capture their underling knowledge graph as opposed to writing a single long book,

We preserve the original multiplicity of possible explanations: From the graph that the expert produces, we are able to produce the infinite number of possible linearizations that the expert themselves could produce, meaning that less creative potential is lost in converting expert knowledge to a graph format than is destroyed when converting expert knowledge into linear text.


The author can produce many possible linearizations, but if the author produces a linearization, we only captures their underlying graph rather than capturing only a few particular traversals like books or articles, then we have richer data from which we can can generate all the possible linearizations of the graph, allowing one reader to go into depth whereas another reader can get a general picture, a potential that would have been lost if the author had just produced one big text.


[motivations/Redundant Indexing]

* Redundant Indexing: Redundant indexing is the idea that [[the different entities of a subject area can be organized in different ways]], and [[Canopy lets you explore the data of a project in these different organizations]].

The different entities of a subject area can be organized in different ways: You could have the battles of a given war organized chronologically, or you could also have them organized by size or length, so we want the same entities with all their details to be available via different organizational schemes.

Canopy lets you explore the data of a project in these different organizations: Canopy lets you define a set of topics with all their details and subcomponents, but also to define arbitrarily many indexes of these topics, listing different groups of topics in different ways, and no matter what manner you arrive at a given topic, you can still burrow into the details and remind yourself of the specifics of the topic before trying to redundantly index it via multiple organizational schemes.


[motivations/Relative addressing]

* Relative addressing: Relative addressing is the idea that rather than [[having categories and category contents]], [[content can itself be the category of further content]], enabling [[a richer addressing system]], related to the idea of [[natural language classification]].

Having categories and category contents: A traditional library, for example, has book shelves which represent categories, and books which are the content being put in those categories.

Content can itself be the category of further content: In Canopy, every paragraph can be the "parent" of several other paragraphs, and so a given paragraph is both like a "book" in that it is content, but also like a "book shelf" in that other information can be put under it as a form of classification, expressing that readers of the first paragraph might want to see the second one.

A richer addressing system: In a traditional library, the more things there are, the harder it is to find each things because the categories get very full and hard to sort through. In Canopy however, because every point can be the "bookshelf" for several other points, as the total number of points grows, so does the "surface area" of addresses at which you can put additional points, and so the number of places to discover content grows as the total size of the corpus grows.


[motivations/Reusability]

* Reusability: Reusability is the idea that instead of [[producing large linear units of explanation]], [[an author can produce reusable blocks of content]], and [[reusable content utilizes the author's time more efficiently]], and [[increases explanation quality]]. But [[how is this different from the internet generally?]]

Producing large linear units of explanation: Generally authors write long books or articles, and readers are reading the entire sequence from beginning to end.

An author can produce reusable blocks of content: If an author produces a reusable explanation of a certain idea, then readers of multiple different explanations are able to access it in the context of their various traversals of the content, as if multiple articles were reusing the same paragraphs, and [[reusable content is similar to spoken explanation]].

Reusable content is similar to spoken explanation: A human explainer will say the same idea or opinion in multiple contexts whenever it comes up, which means their explanation of that idea is "reusable" within their various explanations.

Reusable content utilizes the author's time more efficiently: When an author invests in producing a high-quality explanation of a certain concept, if it is a paragraph in a book, it is only seen by readers of that entire book, whereas with Canopy, many consumers requesting different explanations will see that same content in different contexts, leveraging the author's investment more efficiently.

Increases explanation quality: If the same ideas are written and rewritten as ephemeral content, then they might come to lack quality and nuance, but if an author can reuse a certain explanation many times, they are incentivized to invest in it, and many more people will receive the highest quality explanation of a concept available.

How is this different from the internet generally? The entire internet is composed of articles that make hyperlinked reference to other articles, so how is the reusability of Canopy explanations different than the regular internet? The internet is composed of [[distinct explanations that reference one another]], whereas [[Canopy produces single explanations out of reusable pieces]], which helps [[avoid fragmentation in the mind of the reader]].

Distinct explanations that reference one another: Articles on the internet are distinct explanations with fixed sequence, which happen to reference other such articles. In following a hyperlink, one is changing context and subject matter.

Canopy produces single explanations out of reusable pieces: In Canopy, links refer to elements of the current subject matter, and selecting links displays explanation of those elements, producing one continuous explanation, not a set of distinct, tangentially related explanations, like internet articles.

Avoid fragmentation in the mind of the reader: [[Reading connected paragraphs connects content in the mind]], whereas [[reading disconnected explanations produces disconnected understandings]], and [[what the reader wants is a single merged understanding]]. Canopy thus reduces [[fragmentation]] by enabling [[mergability]].

Reading connected paragraphs connects content in the mind: When a reader reads multiple paragraphs that are organically connected, with some aspect of one paragraph being the subject matter of the next paragraph, the content has a much better chance of becoming connected in their mind, so that they will be able to recall one idea when considering the other.

Reading disconnected explanations produces disconnected understandings: If one explanation sends me by hyperlink to a different location that has a new explanation that isn't explicitly connected to a concrete aspect of the previous one, it will be difficult when thinking about the first to remember that I read the second, and I am likely to develop two siloed understandings of the two articles, with neither bringing to mind the other.

What the reader wants is a single merged understanding: What the reader needs is a composite understanding that merges information from the two explanations, making facts from either one available when they recall the subject matter. This can be done by the reader manually, but it can also be done by the writer in the first place.


[motivations/Rich Compositions]

* Rich Compositions: Rich compositions is the idea that [[descriptions can be deeply nested]], but [[limits of human cognition limit composition size]], and so [[Canopy makes it easier to reconstruct complex compositions]], [[increasing the complexity of ideas that can be successfully communicated]].

Descriptions can be deeply nested: A description of a complex thing might require describing a complex part, which in turn might require describing a complex part, and so on.

Limits of human cognition limit composition size: There are [[limits to working memory size]], and there are [[limits to maximum session length]].

Limits to working memory size: A person can only keep track of a certain number of things at any given time, so a definition that contains a definition that contains a definition might end up overflowing the amount of things the person can keep track of, causing them to fail to assemble the explanation.

Limits to maximum session length: The amount of content that can be communicated in a given session is limited by the listener's attention span, and information communicated in one session might be forgotten by the next, making it difficult to keep all the required subdefinitions in memory long enough to form the desired original picture.

Canopy makes it easier to reconstruct complex compositions: By showing an explanation as a brief paragraph, followed by a brief definition, and so on, a reader can burrow many levels down in the composition of an idea, and yet still retain very clearly on the page the path by which they got there, so that they can later reassemble the pieces they are reading into the original big picture.

Increasing the complexity of ideas that can be successfully communicated: If it becomes easier for authors to communicate to readers complex ideas that require the proper assembly of many complex subcomponents, and if readers become more likely to be successful in reconstructing such complex compositions, then authors and readers might choose to express and consume ideas of greater complexity than are currently communicated.


[motivations/Scale]

* Scale: Scale is the idea that [[fixed mediums limit the maximum size of explanations]], whereas [[non-linear media allows greater explanation size]].

Non-linear media allows greater explanation size: Non-linear media can grow larger than linear media because [[non-linear media is more navigable at large sizes]], and also [[non-linear media isn't constrained by audience interest]].

Non-linear media is more navigable at large sizes: [[Difficulty navigating large corpuses limits corpus size]], so because [[non-linear organization increases the ease of navigating large corpuses]], [[non-linear media enables larger explanations]]. (But [[what about search?]])

What about search? Doesn't the existence of search make it tractable to find things in a large corpus of linear explanations very quickly? Yes, but [[identifying the right search terms and content tagging is difficult]], and [[search doesn't give the reader an organizational scheme]].

Identifying the right search terms and content tagging is difficult: If I am searching for content, I have to predict what keywords it will include, or the author will have to have labeled it properly, and in large corpuses, anticipating how to label content and search for it becomes more difficult.

Search doesn't give the reader an organizational scheme: When the reader accesses the portions of a corpus by search, they are jumping directly to one piece of information, and don't know anything about where it is in the grand scheme of things, whereas if they access the information via a subsuming tree structure, they will have a general idea of what exists, and can use that structure to organize the information in their mind.

Non-linear media isn't constrained by audience interest: Non-linear media can grow large even if people prefer to consume small explanations because [[non-linear explanations decouple storage from presentation]], and so [[demand for content of a certain size won't prevent large corpuses]].

Fixed mediums limit the maximum size of explanations: Authors are constrained in the length of books and articles by [[medium constraints]] and [[audience relevance]], and so even if the author has information they would like to publish, often they have no good place to put it. (And [[even niche works have the same problem]].)

Medium constraints: Practically speaking, a book or article cannot become greater than a certain size, both because there isn't an audience for such large works, and sometimes because of literal constraints on how much content can be printed and circulated.

Audience relevance: Given that most readers of a book or article are reading more or less from beginning to end, an author can't include things that aren't relevant to all readers.

Even niche works have the same problem: You might say [[niche content solves limitations of audience relevance]], but [[niche content is constrained by audience relevance on a smaller scale]], and [[fragmented works create problems of discoverability]].

Niche content solves limitations of audience relevance: You might say when there is a subpopulation that wants more information than the rest of the audience, an author can produce additional chapters, books, or articles just for them, meaning there will always be space for any given piece of information.

Niche content is constrained by audience relevance on a smaller scale: Niche books and articles will also be forced to leave things out, because within the niche readership there will be people with different particular interests, and the author still won't be able to include points that aren't relevant to all readers.

Fragmented works create problems of discoverability: Even if an author could find space for everything they want to share by producing multiple works for different audiences, this produces problems of [[discoverability]] because eventually not everyone will know about the existence of all the various works and what they contain, defeating the purpose of finding a place for information so that it can be shared.

difficulty navigating large corpuses limits corpus size: Even if we are physically able to store large corpuses of information, if we don't have a good way of navigating them, then people aren't going to go to the trouble to assemble that much information in the first place.

Non-linear organization increases the ease of navigating large corpuses: One of the foundational findings of computer science is that [[finding things in a list is slow]], but [[finding things in a tree is quick]], and so if non-linear media organizes content by tree, it will drastically increase the speed at which the reader can find what they are looking for.

Non-linear media enables larger explanations: Because it is more easily navigable at large sizes, non-linear media removes one of the limiting factors on the size of explanations.

Finding things in a list is slow: Finding something in a linear sequence like a book requires scanning the entire sequence, and even tables of contents require scanning the table of contents, which becomes impracticable if there are many things in it.

Finding things in a tree is quick: Imagine an employee is trying to find their name in a company org-chart. All they have to do is find their division, department, and team. Thus, they are able to find their name while only looking at a few nodes in the tree, which is much faster than having to read through a list of every employee name. This is an example of how finding things in trees is faster than finding them in lists.

If tree organization makes it easier for users to find what they are looking for in large corpuses of information, then it will be worth while for authors to produce larger corpuses.

non-linear explanations decouple storage from presentation: With customized explanations, what the author produces and what the reader consumes don't have to be the same thing, and so the author is no longer constrained to produce content in exactly the format in which the reader wants to read it.

Demand for content of a certain size won't prevent large corpuses: Even if readers want explanations of a certain size, authors will not be constrained to produce only that much content, but can produce much more, and let each reader read what they want to.


[Inbox]

Subtopic paragraphs should be cumulative, and when someone gives cumulative descriptions like in an email thread, it is an approximation of subtopic chains.

Explanations of the same thing which comes to exclude different things, eg Chomskian linguistics to exclude a rival theory versus Chomskian linguistics to exclude magic, are two different explanations, or at least, two parts of an explanation.

A series of progressively detailing images.

To convert text to diagraph you have to ask what is it coming to exclude, ie what is the background that the statement is presuming.

If every link is either a dependency or a composition, what is the bidirectional link connecting subordinate topics to parents and vice versa? In a sense both are offered additional context for the other, ie both can be specifications relative to the other. With A you can have AB, and with B you can have AB.

Maybe everything is specification and clarification is just specification of something that you probably already know.

In the global namespace, also use post-phrasing eg "the codebase: The Canopy Codebase has xyz."

People speak about things like motivation questions in students like it is a methodology problem, not a content problem, when you can get pretty far enumerating the questions literally.

Information like a verb table isn't necessarily stored in memory that way, eg irregular verbs might be stored as subpoints to regular ones.

If all follow-up points are done with links, then additional clauses of a paragraph are more clearly "breath" related, whereas otherwise you are using adjacency for multiple things.

Applications of Canopy in programming: The original author of a codebase is often more comfortable maintaining it than are later team members. Canopy can help original authors organize the information they know, such as which parts of the code power which features, where certain decisions are made, and what the original motivations for various design choices were. One could begin at a business requirement, and look at the features that satisfy it, the design decisions made for that feature, and the code that implements it, or one could begin with a line of code and work up, seeing what it does, and why, all the way to the original business requirement.

- The train of thought leading to each test case, from a description of the system under test.

- Applications of Canopy in academia and journalism: Some of the same subjects that are discussed in popularly are also studied in academic institutions. If explanations that were produced in academia were organized differently, it might be more easy for journalists and popular commentators to connect stories of the day to long-term historical or economic analysis, allowing readers who begin at a story to drill into its larger themes, and for students of theoretical ideas to find practical examples of how they play out in the world.

- Canopy can be used as your school notebook for a class, and your goal in reviewing your notes is to come up with an explanation that subsumes all the points made in class, why was each necessary.

- Applications of Canopy in library science: For any set of books, one can imagine a librarian who has read all of them, and could answer questions like "what author addresses this or that point", and without taking sides, that librarian could through quotations put one author in conversation with another, and could list for you all the arguments made in the literature for one position or the other, and the responses, in neutral terms, exerting judgement only in the potential meaning of terms in ordinary language. Therefore, for any set of static resources and a shared language, there exists a set of valid "library diagraphs", which present the indexing of the different parts of those resources by what questions they address and what answers they give, and it might be desirable to produce such diagraphs for popular consumption.

- Giving information in stages creates earlier images that forereference later ones, so that the information is recallable.

Compare diagraph to programming, eg scope, call stack, functional paradigms versus imperative, etc.

Spaced repetition heat map

A source of fragmentation is that experts use generic predication rather than explicit forereference because for _them_ it _is_ a reliable reference.

The way that you sometimes zoom in and sometimes zoom out to analyze a thing

A course is like a diagraph in that everything you mention has to be connected to some previous thing, subsuming even the whole discipline.

We can add invariants like "how do you know" to every point.

Maybe there is a habit of only referencing entities even in prose if they have been imported via link in that paragraph or a direct ancestor.

Overlapping entities like a perek with sugya subtopics and the sugyas as topics, and the differences in how the same information is covered in those two contexts.

If I make a provisional subcategory I can add a note to the parent reminding that it exists and should receive a forereference at some point – ie AT: how do I get visibility into nearby categories that need to be subsumed?

- Maybe when all the children of a point cohere into 4 subcategories I descend.

- Most disambiguation is probably all global references, and maybe even new categories at every level.

I didn't see subcategories could proceed

Canopy codebase: The Canopy codebase contains the Canopy front-end library, the Canopy parser library, and the Canopy command-line interface.

Canopy front-end library: The Canopy codebase contains a front-end library. The front-end code can be found in the repository. The front-end library consumes JSON produced by the parser library which is served by a Canopy server. The code base implements the Canopy project.

The front-end code can be found in the repository: The front-end code can be found in the repository under the `src/client` path.

Canopy parser library: The Canopy codebase contains the backend code for the parser that Canopy uses to convert diagraph script into the JSON that is consumed by the front-end library.

Canopy command-line interface: Canopy has a command-line interface that you can use to set up a Canopy project, convert dgs files into JSON, and run a Canopy server!

Design principles of Canopy: Canopy is designed to mimic the interface of human explanation. The Canopy parser is intended to recognize the same patterns in text as does a human listener.

Mimic the interface of human explanation: Canopy is intended to present the same options to the reader as does a human explainer. The same way that an expert can summarize a domain and then be asked follow up questions, Canopy presents a small amount of information that contains within it follow-up queries that are supported.

The same patterns in text as does a human listener: When a later paragraph references an earlier one, it shouldn't need to be identified with a hypertext reference, because a human reader wouldn't need hypertext to identify the reference as a reference. It must be that the reader recognizes something in the later paragraph they saw in the earlier one, and on this basis recognizes the reference. Canopy is intended to make the same recognition based on the same information.

- Links that reify ordinary language reference - synchronized with no way for links to change from linguistic reference.

The diagraph data structure: There is a definition of diagraph. There are things that make good diagraph. There are different methods of making diagraph. There are implementations of the diagraph data structure.

Definition of diagraph: Diagraph is a graph data structure. Traversals of diagraph are valid prose explanations. A diagraph is composed of a global namespace of topics. Each topic contains a local namespace of subtopic names. One subtopic of the topic matches the topic.

- For each line of the definition, give an example as a local reference.

Good diagraph: Certain qualities make diagraph good.

Methods of making diagraph: There are different methods for making diagraph. You can use the canopy bulk mode for example.

Canopy bulk mode: The canopy bulk mode is a CLI tool for making diagraph.

- Library diagraph and the motivation ie the need for uncontroversial summaries of what views exist without trust for the summarizer.

Implementations of the diagraph data structure: Of which, the Canopy project is one.


Functions of the Canopy library: There are certain functions the Canopy library performs, like run a Canopy server.

Canopy server: A Canopy server is a server that serves and supports the Canopy.js library. The Canopy server can be a static assets server or a node.js script. The server delivers the Canopy.js library on the first request, and then handles subsequent requests from the browser for JSON data files. The Canopy server is necessary to view an example of the Canopy project.

Diagraph script: Diagraph script is a natural language text format that Canopy uses to construct a Canopy website.

Motivations for Canopy: There are problems Canopy is intended to solve. There are interactions Canopy is designed to support. There are reasons why a solution like Canopy should be possible.

- The idea that experts have a massively redundant graph where every topic relates to every other topic.

Problems Canopy is intended to solve: There are certain inefficiencies in the current creation and consumption of explanation that motivate the creation and use of Canopy.

- Canopy can be compared to other solutions to those same problems, wikis, annotation, semantic web, etc.

Interactions Canopy is designed to support: There are certain desirable ways of interacting with explanation that Canopy supports.

Reasons why a solution like Canopy should be possible: Here they are.

THe fact that infinite tree storage has only existed for a bit, and binary search being the most efficient way to look things up.

How to use Canopy: There are instructions for how to produce diagraph with Canopy, and how to use a Canopy website.

How to use a Canopy website: Here is how.

How to produce diagraph with Canopy: Canopy turns a directory of diagraph script files into -


First, it is useful to understand the [ problems Canopy solves]. Then, we can discuss [how Canopy helps], and also specifically, the [ applications of Canopy] in different use-cases. Those interested can inquire into [how Canopy works] on a technical level and understand the [ design of Canopy]. Those wishing to make their own project with Canopy can read about [how Canopy is used] practically speaking. Anyone unfamiliar with the user interface can do a short [walk through]. (Try using the arrow keys.)

Canopy is a JavaScript library for creating and browsing the diagraph data structure. There are functions of the Canopy library. There are motivations for Canopy. There are design principles of Canopy. There are applications of Canopy. There is a way how to use Canopy. There is a Canopy codebase.

- Canopy has features/desiderata? There need to be clause links, so that you can zoom in on a description white paper archive notes
- English as programming language
- Use question topic names.

When does a person use a subtopic eg "France#Economy" and when does a person use a topic


