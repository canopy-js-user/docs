[Canopy]

* Canopy JS:
Welcome! This is a draft of the documentation for the Canopy JS project. You can use Canopy to create interactive explanatory websites like this one. (Try using the arrow keys!)
There are various [[motivations for Canopy]], and different [[applications of Canopy]].
Canopy is used by [[readers]] and [[writers]], and this is an explanation of [[how Canopy works]].
Canopy has a [[codebase]], which you can find on the [project Github page](https://github.com/canopy-js/canopy-js).


[Applications]

* Academia: Canopy would allow academics to produce larger cumulative works: Rather than producing original research on a narrow point, some academics want to express their view of a domain or discipline in its entirety, yet this type of work would be difficult to produce in the format of books and articles, but Canopy makes it easy to produce very large interconnected works.


* Applications of Canopy: There are applications of Canopy in [[journalism]], [[academia]], [[economics]], [[education]], [[software engineering]], [[organizational knowledge]] and [[library science]].


* Economics: Canopy could model complex markets and surface opportunities: As the global economy becomes more and more complex, it can be difficult for beginners to even know what sorts of goods and services are in demand, and so Canopy would enable doing things like modeling a market in all its details, what sorts of firms it includes and what sorts of needs they have, so that job seekers and entrepreneurs can more easily find out about new opportunities.


* Education: Canopy can [[help organize pedagogic content knowledge]], [[offer more clarificatory information]], and [[make accessing prerequisites easier]].

Help organize pedagogic content knowledge: Sometimes there are specific examples or analogies that help strengthen students' intuitions for why a given point is true, but not every teacher might know of every such example, so Canopy can help produce amalgam works that have room to collect all such examples.

Offer more clarificatory information: It isn't always clear to everyone what the author means by a certain statement, but it might be clear to most students, so Canopy allows the author to include as much clarification as they want, allowing a student who is confused to get more explanation without forcing students who understand to wade through it.

Make accessing prerequisites easier: Every point assumes some prior knowledge, but a textbook can't explain every such point, yet some students will be lacking prior knowledge on a given point and will be prevented from understanding, so Canopy lets the author explain required background information only for those students who need it, and not for those students who don't.

Some aspects of teaching are universal, but sometimes a certain topic has specific pedagogic information that is helpful for students, like analogies or examples that provide intuitive reasons why a certain fact is true, yet sometimes it is hard for students to find this type of information, and space limits on linear works make it hard to include everything, but because Canopy enables explanations of larger [[scale]] and the expression of [[explanatory invariants]], those interested can produce pedagogic works that attempt to cumulatively assemble all such knowledge for a discipline.


* Journalism: Canopy would let journalists tap into reusable background content: When a journalist discusses a news story that has larger historical context, Canopy would let them connect their story to reusable background content, allowing readers who are interested to burrow into the historical context without every journalist having to produce it from scratch.


* Library science: Canopy would allow the creation of new types of work that synthesized information from multiple sources, [[showing all the information related to a point in a classic work]], or [[all the words that discuss a given point]], [[without taking any content positions]], [[similar to the speech of a librarian]].

Showing all the information related to a point in a classic work: For a given paragraph in a classic work, librarians could assemble all the commentary on that specific point, and all the commentary on those commentaries, and so on.

All the words that discuss a given point: Rather than start from an existing work, librarians could produce a general description of a domain, and assemble specifications and perspectives from different works on each detail, allowing readers to burrow and discover what discussions exist in each domain, and to access the specific portions of the original sources involved in the discussion.

Without taking any content positions: Such works would never have to take positions on what is actually true, because their job would just be to say what has been said on a given subject, and so as long as everyone agrees on what has been said, it should be uncontroversial to produce such a structure even if the individuals doing it disagree strongly in their content views.

Similar to the speech of a librarian: We can think of librarian speech as being a genre of writing, things that a librarian might tell you about the sorts of works that are available, and which works discuss which points. Just as a librarian would not take positions even on very obvious points, so too would such works merely point the reader in the direction of notable works that discuss a given question.


* Organizational knowledge: Information in an organization is often complex and interrelated, and only specific pieces are relevant for given individuals at any given time, so Canopy's graph format is a natural way to represent such information because it lets users discover related information and context they might not have been aware of, without forcing them to sift through general information not specific to their needs.


* Software engineering: Canopy [[makes it easier to understand the connections between subcomponents of a system]], to [[find reasons for the properties of the system]].

Makes it easier to understand the connections between subcomponents of a system: There are often facts about a system as a whole that don't really belong in the comments for any one class or component, but if they would be put in a big pile then no one would be able to find anything in it, so Canopy creates a new place to put system-wide information, and makes it easier to start from the documentation of one component and navigate organically to new facts about the system as a whole.

Find reasons for the properties of the system: It can be difficult to figure out what business requirement or engineering constraint caused a particular piece of code to be written, but Canopy can document the original user stories and the history of engineering decisions all in one place, allowing maintainers to navigate from a given code component to an explanation of why it is that way.

One of the challenges of code documentation is that there is often no good place to put information about the system as a whole where it will for sure be seen without becoming too long for people to read, or for explaining how a certain piece of the system fits into the system as a whole, but Canopy excels at expressing information about a domain at a high-level and letting users burrow into the details, or starting from a part and zooming out to see its role in the big picture.


[CLI]

* Canopy CLI: The Canopy CLI is a command-line tool for managing Canopy projects. It has various [[commands]].

Commands: There is a [[Canopy CLI init command]], a [[Canopy CLI build command]], a [[Canopy CLI bulk command]] along with a few others.


* Canopy CLI build command: The `canopy build` command takes the [[topic file|topic files]] in the [[topics directory]] and uses them to populate the [[build directory]]. The Canopy CLI build command takes varies [[parameters]]. It must be run from the project directory.

Parameters: There are various optional parameters to the build function. There is the [[project path prefix parameter]].

Project path prefix parameter: The project path prefix parameter lets the user specify the [[project path prefix]]. If the hosting URL will be `www.example.com/myProject/`, then the project should be built like this:
```
canopy build --project-path-prefix myProject

```


* Canopy CLI bulk command: The Canopy CLI bulk command is a tool for creating a large quantity of content at a single time, rather than creating [[topic file|topic files]] individually by hand. The writer runs the `canopy bulk` command, and the project topic files get loaded into the [[Canopy bulk file format]]. The author makes whatever changes they want, and then closes the editor to update the [[topics directory]] with their changes. The bulk command has a [[sync mode]], and it has a [[delayed mode]].

Sync mode: Sync mode is a feature of the [[Canopy CLI bulk command]]. Sync mode lets you open the topics of your project in a bulk file that is watched by the CLI, and changes are continuously processed to update the [[topic file|topic files]] and build the project.

Delayed mode:


* Canopy CLI init command: The `canopy init` command is one of the commands of the [[Canopy CLI]]. The `init` command initializes the project, which entails adding all files and directories expected in the [[project directory]]. It must be run from the project directory.


* Canopy CLI serve command: The `canopy serve` command runs a local server to display your project. It must be run from the project directory. The server runs on port 4001 by default. The `serve` command takes various parameters.


* Canopy bulk file format: A Canopy bulk file is composed of sections, each of which begins with a [[category path header]] followed by [[topic files]] or [[note files]].

Category path header: In a Canopy bulk file, a section begins with a square-bracket enclosed category path like [Category 1/Category 2/Category 3] on is own line, followed by a double newline. The writer uses spaces and not underscores, although the resulting directories will have underscores.

Topic files: Below a category path header you can have a topic file, which is an asterisk and space followed by desired contents of a topic file. Canopy bulk mode will look at the key of the topic file and use it to infer the appropriate name for the [[topic file]].

Note files: Below a category path header you can have notes, which are [[block|blocks]] not preceded by an asterisk, or a series of blocks that have an asterisk before them, but no [[key]] on the first line. In such a case, these blocks are added to the [[category notes file]].


[Code]

* Codebase: The Canopy codebase is composed of the [[Canopy CLI code]] and [[Canopy client code]].


[Code/CLI]

* Canopy CLI Code: The CLI code uses the Commander.js library and is composed of several command functions.


[Code/Client]

* Canopy client code: The Canopy front-end client is composed of several major components.


[Data model]

* Canopy Data Model: Content in Canopy is composed of data in the form of [[category|categories]], [[topic|topics]] and [[subtopic|subtopics]].


* Canopy Path: A Canopy website displays a currently selected path. The path determines which paragraphs are displayed, and is also visible in the [[URL]]. The path is composed of [[path segment|path segments]]. The path begins with the [[root topic]].

URL: A Canopy path is displayed as a URL by joining each [[path segment]] separated with a forward slash, and if the path segment has a subtopic, it is appended to the topic name with a leading pound sign, for example, `A/B#C/D`.


* Category: A category is an entity in the [[Canopy data model]] that helps organize [[topic|topics]]. Categories can contain other categories, and can contain topics. A top-level category is a category that is not inside any other category. Categories are [[merely an organizational tool]]. A category is represented on disk by a [[category directory]], and the category directory may contain a [[category notes file]].

Merely an organizational tool: Categories exist to help the author organize their [[topic|topics]], and categories do not affect the front-end behavior in any way. A user navigates to a topic directly, and then follows links to other topics, regardless of what categories those topics happen to be in. The URL path eg `A/B/C` does not represent a category hierarchy, but rather the chronology of the user's path through the various topics they've interacted with.


* Category Notes File: A category notes file is a file inside a [[category directory]] whose name follows the category name. A category notes file is for notes related to the category that do not yet have a permanent topic. Even if a category notes file has a valid topic key, unless that key matches the filename, the build process will ignore the file. This is to prevent false-positives where category notes begin with a question that is mistakenly interpreted as a topic key.


* Category directory: A category directory is the on-disk representation of a Canopy [[category]]. It is a simple directory in the [[topics directory]], and has no special behavior. The subdirectory structure of the [[topics directory]] is ignored by the build process and all [[topic file|topic files]] are processed as a flat list.


* Default Topic: A default topic is the topic of the project that the user first sees if no specific path is selected. Every project must have a default topic, chosen when the project is initialized. The default topic is specified in the `project/canopy_default_topic` file, which contains a path to the default topic's `expl` file in the `topics` directory.


* Global reference: A global reference is a link to a [[topic]], as opposed to a [[local reference]]. Selecting a global reference can inline the target topic paragraph onto the page below the current paragraph, or the reader can redirect to the target paragraph as the new root of the page.


* Import reference: An import reference is a link to a [[subtopic]] of a different [[topic]]. Import references must be near a [[global reference]] to the topic of the target subtopic, and selecting the import reference displays the path from that topic to the subtopic in question. An import reference can sometimes be converted into a [[global reference]] by converting a subtopic to a [[subordinate topic]].


* Local reference: A local reference is a reference to a [[subtopic]], as opposed to a [[global reference]]. Selecting a local reference inlines the target subtopic paragraph onto the page below the current paragraph.


* Orphan Subtopic: An orphan subtopic is a [[subtopic]] that has a valid [[subtopic name]] and [[subtopic paragraph]], but which is not connected to the subtopic's [[topic]] via any [[subtopic path]]. Orphan subtopics are ignored by the build process because they are inaccessible to the user.


* Path segment: A path segment is a unit of a [[Canopy path]]. It is composed of a [[topic]], and an optional [[subtopic]]. The path segment represents the [[subtopic path]] from that topic to that subtopic.


* Root paragraph: The root topic paragraph is the first paragraph on the page, and is the [[topic paragraph]] of the [[topic]] of the first [[path segment]].


* Root topic: A root topic is the topic that an explanation begins from. It is the topic of the first [[path segment]], and is the topic of the [[root paragraph]].


* Subordinate topic: A subordinate topic is a topic that is usually only accessed via a global reference from a more frequently seen topic. Subordinate topics are contrasted with [[subtopic|subtopics]] as a way of expressing an idea that is subordinate to another. In general, subtopics should be used for ideas that cannot be explained without first having explained another, whereas subordinate topics can be used for ideas that can be explained independently, even if they require the context of a prior idea to be fully understood.


* Subsumed Subtopic: A subsumed subtopic is a [[subtopic]] of a [[topic]] that is connected to the [[topic paragraph]] via a [[subtopic path]], unlike an [[orphan subtopic]].


* Subtopic: Subtopics are a major type of entity in the Canopy data model. Subtopics belong to a [[topic]]. A subtopic models an entity or idea that requires additional context before it can be explained, as opposed to a [[topic]]. Because a subtopic requires context, it is displayed at a [[subtopic path]]. Sometimes information modeled using a subtopic should instead be modeled with a [[subordinate topic]]. It is important to know [[when to use topics versus subtopics]].


* Subtopic name: A subtopic name is a string that represents the name of a [[subtopic]].


* Subtopic paragraph: A subtopic paragraph is the paragraph that belongs to a [[subtopic]].


* Subtopic path: A subtopic path is a path beginning from a root topic paragraph, and following its local references to subtopic paragraphs, and their local references to subtopic paragraphs, until reaching the target paragraph. A subtopic path is one branch of a [[topic tree]].


* Topic: Topics are the fundamental organizing units of information in a Canopy project. A Canopy topic is an explanation of some idea or entity. A topic is an entity that can be explained without prior context, as opposed to a [[subtopic]]. A topic in Canopy has a [[topic name]], and a [[topic paragraph]], whose [[subtopic|subtopics]] form a [[topic tree]]. The data of a topic is stored in a [[topic file]]. Every project must have a [[default topic]]. It is important to know [[when to use topics versus subtopics]].


* Topic name: A topic name is a string that represents the name of a Canopy topic. The topic name is defined by the [[key]] of the first block in a [[topic file]]. The topic name is the target of [[Global reference|Global references]].


* Topic paragraph: A topic paragraph is the paragraph associated with a given [[topic]]. The topic paragraph has [[local reference|local references]] to [[subtopic|subtopics]] that have [[subtopic paragraph|subtopic paragraphs]], forming a [[topic tree]].


* Topic tree: A topic tree is the tree formed by a [[topic paragraph]], which contains in it [[local reference|local references]], which reference [[subtopic|subtopics]] which have [[subtopic paragraph|subtopic paragraphs]], which also contain local references, which connect the subtopic paragraphs to further subtopic paragraphs, forming a tree.


[Mechanics]

* How Canopy works: Canopy takes [[explanation files]] and [[uses them to produce build data]], which [[the front-end client uses to build a web interface]] and [[the user interacts with it]].

Uses them to produce build data: The build process produces JSON files that tell the front-end client how to render the paragraphs of the project.

The front-end client uses to build a web interface: The web interface requests the default topic, and uses it to render the initial page.

The user interacts with it: When the user select a new link, the Canopy web interface rerenders the page.


[Mechanics/Disk]

* Block: A block is a unit of information in an [[topic file]]. Blocks are multi-line units of text separated by double newlines. If the first block of the file has a [[key]], it is the [[topic definition]]. If a block after the first has a key, it is a [[subtopic]] definition, so long as it is a [[subsumed subtopic]]. If there is no key, the block defines a [[note]].


* Key: A key is an optional prefix for the first line of a [[block]]. A key is a string that ends in a `:` or `?` character. The key represents The presence of a key indicates that a [[block]] is a [[topic]] or [[subtopic]] definition, as opposed to a [[note]], unless the block is an [[orphan subtopic]], which has a key but nevertheless functions as [[note]]. The key specifies the [[topic name]] or [[subtopic name]]. [[A key can be given on its own line]].

A key can be given on its own line: Eg you can have:
```
Key:
Sentence 1
Sentence 2
Sentence 3

```


* Note: A note is a block of a topic file that does not begin with a key and so is ignored by the build process. Notes can be used to express information that will later be converted into proper [[paragraph node|paragraph nodes]].


* Paragraph node: A paragraph node is a [[block]] that defines a [[topic]] or [[subtopic]].


* Topic definition: A topic definition is a type of [[block]] in a topic file that defines a [[topic]].


* Topic file: A topic file is a file stored in the [[topics directory]] that defines a [[topic]] of a Canopy project. Topic files end in a `.expl` extension. Topic files have a certain [[file format]].

File format: A topic file is composed of [[block|blocks]].


[Mechanics/Settings]

* Project Path Prefix: A project path prefix is a string that comes after the domain name of your hosting provider. For example, if you will be hosting your project on `www.example.com/myProject/Topic_1`, then the project path prefix is "myProject". If you are hosting your project on `www.example.com/A/B/Topic_1`, then your project path prefix is `A/B`. Canopy needs to know about this string in order to determine what portion of the URL is the current [[page path string]]. The project path prefix is specified via the [[project path prefix parameter]] to the [[Canopy CLI build command]].


[Project Directory]

* Build directory: The build directory is a directory that gets created by the [[Canopy CLI build command]], or which the author can provide in the case of a custom build. The build directory includes a [[data directory]], an [[HTML index file]], and a [[Canopy JavaScript asset]].


* Canopy Javascript asset: The Canopy Javascript asset is a file called `canopy.js` that contains the front-end client code. The `canopy.js` file is expecting to be loaded on a page that has an HTML element with a `_canopy` id, which it will fill with content.


* Canopy Root Element: The Canopy root element is a DOM element that has the id `_canopy_`. If the user is using the standard [[Canopy CLI build command]], then this element is created automatically in the [[HTML index file]]. The Canopy root element is expected to have several [[data attributes]].

Data attributes: The Canopy root element is expected to have:
1. A `default-topic` data attribute, which gives the mixed-case topic name of the [[default topic]]
2. A `project-path-prefix` data attribute, which gives the [[project path prefix]]
3. A `hash-urls` data attribute, which indicates whether Canopy should use the hash URLs feature.
Only the `default-topic` attribute is strictly mandatory.


* Data directory: The data directory is a directory that the [[Canopy CLI build command]] creates inside the project's [[build directory]]. The data directory is named `_data` (because there might be a topic in the project called `data`.)


* Default topic specifier file: The default topic specifier file is a file called `canopy_default_topic` that exists in the root level of the [[project directory]]. The `canopy_default_topic` file contains the [[file path]] of the [[topic file]] for the project's [[default topic]].

File path: The file path of the [[default topic]] should be given relative to the root of the project directory, eg `topics/Category_A/Topic.expl`.


* Explanation files: Explanation files are files ending in the `.expl` extension. Explanation files are the [[topic file|topic files]] that live in the [[topics directory]].


* HTML index file: The HTML index file is an `index.html` file that is created in the [[build directory]] either by the [[Canopy CLI build command]], or by the user in the case of a custom build process. The `index.html` file must include the [[Canopy Javascript asset]], and it must have a [[Canopy root element]].


* Project Directory: The project directory is a directory that contains the data for a Canopy project. The project directory contains the [[topics directory]], the [[default topic specifier file]], and the [[build directory]] when the project has been built.


* Topics Directory: The topics directory is a top-level subdirectory of the [[project directory]] that contains the project's [[topic file|topic files]].


[Readers]

* Keyboard shortcuts: You can navigate Canopy using the [[arrow keys]], [[the return key]], using [[z for zoom]], [[d for duplicate]], [[tab for iteration]], and [[escape for deselect]].

Arrow keys: Users can navigate a Canopy project using the arrow keys (or their vim-key equivalents.) `Up` returns to the current paragraph's parent link, `left` and `right` iterate through the siblings of the selected link, and `down` selects the first link of the currently selected paragraph. Pressing `shift-up` goes to the parent of the previous topic.

The return key: Pressing the return key when selecting a [[global reference]] or an external link will redirect the page to the link target.

Z for zoom: Pressing `z` will "zoom" the path to the lowest order [[path segment]]. For example, if the user has iterated through many topics and is currently reading about the topic "T" and its subtopic "S", then pressing `z` will redirect the user to the page for T with the path to S selected, which can be useful for sharing a link.

D for duplicate: Pressing `d` will open a new tab open to the same path.

Tab for iteration: Pressing `tab` will iterate through the subtopics of a topic in a depth-first search.

Escape for deselect: Pressing `escape` will deselect all links and display the page's root paragraph.


* Mouse interactions: To navigate by mouse, you can [[click local references]] and you can [[click global references]].

Click Local References: If you click a [[local reference]], the target paragraph will be inlined below the current paragraph.

Click Global References: If you click a [[global reference]], if alt/option is not pressed, the target paragraph will be inlined below the current paragraph, and if alt/option _is_ pressed, then the page will redirect to the target paragraph as the new page root.


* Page path string: The page path string is the portion of the URL that indicates the current path of the reader. If the user began on the page for `A`, and then opened a subtopic of `A` called `B`, then opened a [[global reference]] to a topic called `C`, the reader's page path string would be `A#B/C`.


* Readers: Readers use Canopy by navigating to a Canopy website, reading the [[root paragraph]] and [[selecting links]] to explore additional content.

Selecting links: Readers can select links using [[mouse interactions]], or [[keyboard shortcuts]].


[Roadmap]

* Roadmap: There are various features that would be nice to have, such as [[Electron builds]].

Electron builds: It would be nice to have a build option that wraps a Canopy website in an electron app so that people can offer downloadable assets for a project.


[Theory]

* Theoretical questions: There are various theoretical questions related to Canopy.


[Writers]

* Writers: Writers use Canopy by [[creating a Canopy project]], [[adding content to their project]] that follows [[best practices]], [[building their project]], [[serving their project locally]] and [[deploying the project]].

Creating a Canopy project: Creating a Canopy project involves [[creating a project directory]], [[initializing the project]], and optionally [[initializing version control]].

Creating a project directory: Creating the project directory is done by creating a directory, eg running `mkdir my_project` on a Unix-like system. There is nothing special about the type of directory.

Initializing the project: To initialize a project, enter the project directory (`cd` on a Unix system), then run `canopy init`. The [[Canopy CLI init command]] creates various files and folders that are necessary for your Canopy project. Initializing the project entails selecting a [[default topic]].

Initializing version control: At this point you might want to run a `git init` in your project directory and store your project on a service like Github. Storing your project on a hosting service like Github makes it easier to host and deploy your project when it changes.

Adding content to their project: Content is added to a Canopy project by creating files that follow the [[Canopy data model]] and using [[Canopy markup]]. There are several [[methods of producing content]]. Authors can follow [[best practices]] in deciding how to model their ideas in the Canopy data model.

Methods of producing content: Content can be produced by [[manually creating topic files]], or using the [[Canopy CLI bulk command]].

Manually creating topic files: If you create a file in your `topics` directory like `project/topics/category/topic.expl`, and you add a root paragraph to the file like this:
`
` Topic: This is a paragraph for the topic "Topic."
`
Then, when you build your project, this topic can be viewed in the web interface.

Canopy markup: Typical markdown-style styling is available, such as lists, code blocks, tables, footnotes, block quotes, and styling characters. Inline HTML is supported. Asterisks indicate bold and underscores indicate italics.

Building their project: Building your Canopy project entails taking the `expl` files and generating JSON from them that can be used by the front-end client. You can build your project by navigating to your project directory and running the [[Canopy CLI build command]].

Serving their project locally: To serve your Canopy project locally, navigate to your project folder and running the [[Canopy CLI serve command]].

Deploying the project: There are various ways of deploying a Canopy project to an external hosting provider. Canopy's use of [[cosmetic paths]] prevents it from being hosted on a simple static assets server, however, there are several ways of overcoming this limitation and [[hosting a Canopy project]], and there are resources for [[continuously building a Canopy project]].

Cosmetic paths: When reading a Canopy website, the URL adapts to reflect the path of topics and subtopics that the reader has navigated through. The URL of the page might for example read `www.example.com/Topic_1/Topic_2`. However, if the site is hosted on a static assets server, the client will request a directory `Topic_1` that contains a directory `Topic_2`, neither of which exists, when really we just want to serve the root `index.html` asset regardless of the requested path.

Hosting a Canopy project: In order to get around the problem of cosmetic paths, there are four ways of hosting a Canopy project, [[dynamic server]], [[hash URLs]], [[symlinks]], or a [[custom static hosting provider]].

Dynamic server: The simplest way of avoiding the problem of cosmetic paths is to not use a static assets server. The Canopy CLI ships with a dynamic Express server that will ignore request paths and serve the root `index.html` file for all requests. The downside of this option is that dynamic hosting is often more expensive than static assets hosting.

Hash URLs: Canopy has a build option called "hash URLs" that causes the project to prepend all paths with `/#/`, making it irrelevant to the server what comes after the hash symbol. The downside of this approach is that it adds an extra character to all URLs.

Symlinks: Symlinks is build option that creates directories for every topic in the project and symlinks to every other topic directory, each of which contains the same `index.html` asset, so requests like `www.example.com/Topic_A/Topic_B` follow a valid directory path via the symlinks. The downside of this approach is that most hosting providers do not permit symlinks, and creating the directories and linking them is an O(N<sup>2</sup>) operation at build-time.

Custom static hosting provider: Some static asset hosting providers allow the webmaster to specify a redirect when a user requests a non-existent resource. This is the recommended option, and an example of this set up can be found in the [Canopy Github's "example workflows" repo](https://github.com/canopy-js/example-workflows).

Continuously building a Canopy project: You might want to store your Canopy project on Github and trigger builds whenever an `expl` file is changed. Template Github workflows for continuously building a Canopy project hosted in a Github repository can be found on the [Canopy Github's "example workflows" repo](https://github.com/canopy-js/example-workflows).


[Writers/Best Practices]

* Best practices: There are various best practices for making content with Canopy, such as [[when to use topics versus subtopics]], [[when to repeat versus reference content]], [[thinking about entities and relations rather than explanations]], [[creating hyperlinks that follow sentence syntax]] and [[representing different versions of the same idea for different purposes]].


* Creating hyperlinks that follow sentence syntax: It is common to find [[hyperlinks that do not match sentence syntax]], but it is recommended to instead [[create intermediary syntactic subtopics]].

Hyperlinks that do not match sentence syntax: If I have the sentence "Bob lives on 4th Street", and I make the word "street" into a hyperlink to the topic of "streets" generally, then this is a hyperlink that does not match sentence syntax, because in our sentence, "4th Street" is one unit, and it is a name, and so the word "street" is not actually being used as the generic term, (like in the sentence: "I prefer streets to alleyways,") and so if one hyperlinks "4th `street`" to the generic topic "street", then there is a mismatch between how the term is being used syntactically and what the hyperlink is pointing to, which can be confusing to the reader.

Create intermediary syntactic subtopics: If I have the sentence "Bob lives on 4th Street," and I am worried that someone will not know what a "street" is, and I want to link them to that concept, then rather than linkifying the word "street", I can create a subtopic called "4th Street", and make "Bob lives on `4th Street`" a local reference to that subtopic, and in the [[subtopic paragraph]] I can say "4th Street is a `street`", and put the hyperlink in the subtopic, and thus, I am able to offer the reader a definition of the word "street", without creating any hyperlinks that are mismatched to sentence syntax.


* Representing different versions of the same idea for different purposes: Sometimes there will be multiple explanations of the same things appropriate for different times or levels of interest, and this can be modeled using one overarching topic with subtopics for what purpose or level of depth is desired, and these can be referenced using an [[import reference]].


* Thinking about entities and relations rather than explanations: A good practice in Canopy is instead of designing what "explanations" ie paths through the content you think should exist, first choosing what entities exist in the corpus, and what relationships they have, and then letting paths and explanations emerge via these relations, even ones the author didn't themselves anticipate.


* When to repeat versus reference content: Not every idea that is mentioned in multiple places should be referenced by link, because sometimes the author is trying to define an idea for a local purpose, and not send the reader meandering to a new topic and all its relations. In such a case, the author might repeat information that can be found elsewhere as subtopics of a given topic, so that the reader only sees that which is necessary for the current point being made, (and if the reader chooses to read more, a hyperlink to the topic proper can be included in parentheses, indicating optionality)


* When to use topics versus subtopics: It is often ambiguous whether to represent a tangential idea as a [[subtopic]] or a [[subordinate topic]]. In general, if something could be explained in its own right without first explaining the original idea, even if no reader will end up doing so, it makes sense to model it as a subordinate topic. If something cannot be explained without first mentioning another idea, it should be a subtopic. Another rule of thumb is that names that contain other names should be subtopics, eg "the North entrance to Newark airport."


[motivations]

* Motivations for Canopy: The motivations for Canopy include [[observations about human explanation]], [[limitations of conventional explanations]], [[specific features of Canopy]], [[qualities of Canopy content]], and [[new possibilities Canopy enables]].

Specific features of Canopy: Canopy is motivated by specific desirable features such as [[recursive clarification]], [[inline inclusion]], [[explicit state]], [[idea addressing]], [[natural language classification]], [[many small categories]], and [[relative addressing]].

Qualities of Canopy content: Canopy is motivated by qualities of Canopy content, such as  [[customization]], [[clarity]], [[reusability]], [[mergability]], [[proportionality]], [[memorability]], [[informed abridgement]], [[explicit prerequisites]], [[discoverability]], [[hierarchy]], [[exhaustiveness]], [[non-rival media]], and [[connected synonyms]].

New possibilities Canopy enables: Canopy is motivated by new possibilities the system enables, such as [[scale]], [[explanatory invariants]], [[holistic review]], [[pervasive concepts]], [[redundant indexing]], [[prescriptive understandings]], [[rich compositions]], [[interconnection]], and [[completeness]].

Limitations of conventional explanations: Canopy is motivated by limitations of existing explanatory formats, such as [[document metaphor]], [[undersupply]], [[gaps]], [[fragmentation]], and [[Anti-Marginalia]].

Observations about human explanation: Canopy is motivated by observations about human explanation, such as [[realism]] and [[interface of the expert]].


[motivations/Features]

* Explicit state: [[Explanation has state]], but [[linear prose makes it unclear what the state of the explanation is]], whereas [[Canopy makes the state of the explanation visibly clear]].

Explanation has state: In the course of an explanation, we are at a particular point, there are prior points we are developing, and there are further specifications we could make, and a listener needs this information in order to understand what the explainer will say next.

Linear prose makes it unclear what the state of the explanation is: In linear prose, as I read a given paragraph, I can't easily see how we got to this point, or in what directions we can go, because our early context is scattered throughout the earlier portions, and the possible avenues of specification are scattered throughout the later portions.

Canopy makes the state of the explanation visibly clear: In Canopy, the screen doesn't show all prior content the reader has read in a given session, but only the path of paragraphs that explains how they got to the current subject, and links on the page describe all the directions the reader can go now given their position, either adding detail to the current paragraph, or regressing to an earlier idea and adding more detail to it.


* Idea addressing: Idea addressing is that rather than having [[the unit of information]] be [[the category]] or [[the work]], [[we can address individual ideas]].

The unit of information: An information system is broken up into units that can be spoken about and recommended to other people individually. I can recommend a book or a chapter of a book, but it is hard to recommend a specific paragraph because it has no "name" in the system.

The category: In a traditional library, one unit of information is the category, ie the collection of works on a certain topic. The category has a name, so that people can communicate about it effectively.

The work: In a traditional library, one unit of information is the work, like a book or periodical. The work has a name so that people can communicate about it effectively.

We can address individual ideas: In Canopy, every idea gets a paragraph, and every idea gets a name, and from that name the idea gets a URL, and so we can speak about ideas and recommend explanations on a much finer-grain basis.


* Inline inclusion: Inline inclusion is the idea that [[natural language references are not semantically redirects]], yet [[in most hypertext systems references cause redirects]], so [[in Canopy references are inlined by default]].

Natural language references are not semantically redirects: When in conversation I mention something, my intention is usually not to suggest that we change the subject and discuss the idea for its own sake, but rather that we include a brief description of it here so that we can use it to clarify an idea we were already discussing.

In most hypertext systems references cause redirects: In most hypertext systems, content is composed of documents that have references to other documents, and selecting a reference causes the reader to redirect to another document, "changing the subject" to a discussion of the new idea in its own right, rather than as a means to understand the previous subject.

In Canopy references are inlined by default: When selecting a link in Canopy, by default the paragraph associated with the selected link as added to the current page under the paragraph of the link. This allows the reader to retain their place in the earlier explanation, and peruse an explanation of the new idea only insofar as is necessary to continue reading the earlier paragraph. This user experience is more similar to the natural experience of listening to conversation, and so there is a better chance that the reader will be able to assimilate the given information correctly.


* Many small categories: Many small categories is the idea that [[the number of categories in a conventional classification scheme is limited]], which causes [[overly large categories]], whereas [[categories in Canopy can be deeply nested]].

The number of categories in a conventional classification scheme is limited: Most organizational systems have categories which contain subtcategories, and the subcategories may contain subcategories, but there will only be 2-4 levels of nested subcategories, after which things are added to the subcategories in an undifferentiated list.

Overly large categories: When categories can only be nested 2-4 levels, if the corpus is large, then the size of categories will get very large also, and if a category is very large, it becomes intractable for the reader to see the whole list and it becomes random what content they see or don't see when navigating to a category.

Categories in Canopy can be deeply nested: In Canopy, categories can be deeply nested because the content itself is the categorization scheme, and so categories of content can be nested very deeply, and thus there can be very many categories, each of which contain only a few things, so the user who navigates to a given category will be able to see everything it contains and it will not be random which contents get seen or not seen, producing a more precise experience for the reader.


* Natural language classification: Natural language classification is the idea that [[people use natural language as a classification scheme in conversation]], and [[Canopy follows the natural classification scheme]].

People use natural language as a classification scheme in conversation: When someone says "there is a country called France and there are certain exports that it produces economically," the speaker has specified a category in which further information can be put. Similarly, when requesting information, a person might give a brief explanation and then pose a question about something that was mentioned. Thus, the introductory sentences themselves are functioning as a classification scheme for storing and requesting information.

Canopy follows the natural classification scheme: By having the reader navigate by reading paragraphs and selecting certain portions of them, the user is using the natural language content as the form of navigation, similar to how they would request information in a conversation.


* Recursive Clarification: Recursive clarification is the idea that [[explainers must provide conceptual prerequisites]], and whereas the most common solution is to use a [[topological sort]], there are [[downsides to topological sort]], and so Canopy allows readers to [[access prerequisites through recursion]], and there are [[benefits to using recursion]].

Explainers must provide conceptual prerequisites: Most concepts require other concepts in order to understand them, and so when an explainer wants to explain something, they're also going to need to impart all the various concepts that are prerequisite for the given idea.

Topological sort: Topological sort entails taking a set of ideas and their prerequisites and producing a linear ordering in which every prerequisite comes before every idea that requires it. So, the explainer would explain all the simpler ideas first, and only later would attempt to explain the more complex ideas that require the simpler ones. This is the approach followed by most books and lectures.

Downsides to topological sort: Two downsides of topological sort is that it [[produces redundant explanations]], it [[produces unnecessary explanation]], and that it fails to communicate [[the motivation for prerequisites]].

Produces redundant explanations: If all the prerequisites of every idea are front-loaded, then if the reader reads multiple ideas that require the same prerequisite, the reader will be shown that prerequisite every time, even if they already know it.

Produces unnecessary explanation: If all the prerequisites of every idea are always shown before the given idea, then a reader who already knows some of them is forced to wade through information that they already know in order to find the information they don't.

The motivation for prerequisites: If prerequisites are given before the ideas that require them, then the reader isn't entirely clear on why they are learning the idea, as opposed to trying to learn the complex idea, failing due to the lack of the prerequisite, and then going on a tangent to learn the prerequisite in order to learn the given idea.

Access prerequisites through recursion: In Canopy, if a concept requires other concepts, they are not "front-loaded", but rather are referenced as if the reader already knows them, and if the reader doesn't, they may burrow into additional explanation of those prerequisites, and the prerequisites of the prerequisites, etc.

Benefits to using recursion: If the writer assumes the reader has the required background knowledge and the reader must opt in to further explanation, then there is never a problem of over-explaining. Also, because the complex idea was shown first, it is clear to the reader why they are learning the required prerequisite and they will have motivation to learn it.


* Relative addressing: Relative addressing is the idea that rather than [[having categories and category contents]], [[content can itself be the category of further content]], enabling [[a richer addressing system]].

Having categories and category contents: A traditional library, for example, has book shelves which represent categories, and books which are the content being put in those categories.

Content can itself be the category of further content: In Canopy, every paragraph can be the "parent" of several other paragraphs, and so a given paragraph is both like a "book" in that it is content, but also like a "book shelf" in that other information can be put under it as a form of classification, expressing that readers of the first paragraph might want to see the second one.

A richer addressing system: In a traditional library, the more things there are, the harder it is to find each things because the categories get very full and hard to sort through. In Canopy however, because every point can be the "bookshelf" for several other points, as the total number of points grows, so does the "surface area" of addresses at which you can put additional points, and so the number of places to discover content grows as the total size of the corpus grows.


[motivations/Limitations]

* Anti-Marginalia: Anti-marginalia is the idea that while [[many hypertext systems use the visual metaphor of marginalia]], [[marginalia producing splintering]], [[marginalia has no forward references]], and [[marginalia follows the document metaphor]], whereas [[Canopy incorporates commentary into the primary content]].

Many hypertext systems use the visual metaphor of marginalia: Various systems have based their model of interconnections between ideas on having a traditional article but with additional links and comments on the margin, reminiscent of notes made in the margin of a book. This approach represents a view in which articles are the primary content, and "commentary" is something different and secondary, and it resides at various locations on the primary content.

Marginalia producing splintering: If every point in an article has commentary, and pieces of commentary have commentary, then the reader

Marginalia has no forward references: In a marginalia model, commentary references the original work, but the original work obviously has no inherent references to the commentary on itself, and this means that if one reads the original work, and then reads the commentary, it is very possible that the reader will end up with a fragmented understanding where the idea of the comment brings to mind the original article, but thinking about the original article doesn't bring to mind the commentary, because there was no forward reference in the article to the commentary, as opposed to Canopy where primary content references commentary.

Marginalia follows the document metaphor: Marginalia follows the idea that content should be linear documents, and that interactivity and non-linearity is something to decorate it with on the edges, and thus the article and the commentary are represented as two different types of thing.

Canopy incorporates commentary into the primary content: In Canopy, there is no division between primary content and commentary, so an idea would have a paragraph, and that paragraph would mention the existence of more detail, and also the existence of commentary, and the reader would be able to burrow into either, and both types of content are presented as continuations of the earlier content, as not as if they were a different kind of content.


* Document Metaphor: Document metaphor is the idea that [[existing hypertext systems retain the metaphor of documents]], but [[there are shortcomings to the document metaphor]], and so [[Canopy replaces documents with graph traversal]], and [[is itself a generator of documents]].

Existing hypertext systems retain the metaphor of documents: Hypertext systems like the modern internet are modeled as linear lists of paragraphs ("documents") that contain references to other documents.

There are shortcomings to the document metaphor: Hypertext composed of documents is [[less customizable]] and [[more disjointed]] than Canopy explanations.

Less customizable: Hypertext composed of documents does have more ability for [[customization]] than linear text like a book, but because it is itself composed of large blocks of linear text, the customization is less fine-tuned, and explanations shorter than a document cannot be produced, even if not every reader wants to read an entire document.

More disjointed: When a reader traverses a network of hyperlinked documents, the experience is very different from a human explanation. Each document is a new explanation, and so the reader is constantly starting new explanations, and, when a link is opened, the user hasn't finished reading the first document, and so is simultaneously at one spot in the first document and in the second, and may forget the context of the first by the time they return, unlike Canopy where the explanation is composed of small paragraphs so the user always finishes the previous one before starting a new one.

Canopy replaces documents with graph traversal: Instead of making documents the smallest unit of explanation, in Canopy the unit of explanation is the single-sentence paragraph, and paragraphs get woven together to produce a single explanation, not a series of interlinked explanations, similar to how human conversation and explanation is continuous and not composed of disjointed pieces.

Is itself a generator of documents: Not only isn't Canopy a hypertext composed of fixed documents, Canopy is a definition of what makes a valid document, ie, paragraphs in a prose explanation are valid if every a later paragraph specifies a point made in an earlier paragraph like paths in Canopy, and so Canopy is a system for producing valid linear documents of explanation, rather than itself being a hypertext composed of such documents.


* Fragmentation: Fragmentation is the idea that some systems of information storage unintentionally [[separate things that are connected]], which [[harms discoverability]], and [[harms the development of mastery]], [[producing fragmented understandings in the minds of readers]]. Fragmentation is the opposite of [[interconnection]].

Separate things that are connected: Systems that store linear information as linear prose or verbal lectures create fragmentation because [[when storage is coupled to presentation]], [[information will end up far from related information]].

When storage is coupled to presentation: If people only read from books and articles, then the way we store information will be by storing it as books and articles, and so the amount of information and interconnections we can store is constrained by the size and audience of the book, and so we are limited by the fact that the format of storage and the format of presentation have to be the same.

Information will end up far from related information: If understanding of a news story could benefit from an understanding of a historical fact, but not everyone would be interested in hearing about it, then the news story will be stored in a newspaper, and the historical fact will be stored in a textbook, and either the reader of the newspaper won't know about the historical fact, or at the very least there will be great friction in going from the newspaper to the historical fact.

Harms discoverability: If two pieces of information that are objectively related are stored in different places, then people who see one will not necessarily hear about the other, and at best it will be a missed opportunity for offering something of interest, and at worst the reader may form a completely incorrect understanding due to the missing fact.

Harms the development of mastery: If [[mastery requires interconnection]] and [[explanations are siloed]], then [[only experts will achieve any mastery]].

Producing fragmented understandings in the minds of readers: The reader of disconnected media will by default end up with the same disconnections in their understanding as exist in the media itself.

Mastery requires interconnection: Some portion of mastery over a subject matter is knowledge of the combinatorial interrelations between the pieces, such that if you see any part in any context, you can recognize the concept and connect it to the whole.

Explanations are siloed: If different explanations are stored in different books and articles, then they are in "silos," and someone reading one might not hear about information in another.

Only experts will achieve any mastery: Only experts will achieve mastery because [[an expert can overcome fragmented media]] but [[a casual reader retains media fragmentation]], and so [[non-experts cannot reach mastery over even small units]], [[creating large discrepancies in knowledge]].

An expert can overcome fragmented media: An expert might read all the works of a given domain many times, and so the fact that the information is stored in disconnected places is less of a problem because the expert will eventually see everything and connect it all in their mind, even if it isn't connected in the media itself.

A casual reader retains media fragmentation: A casual reader will have [[a fragmented understanding of the domain as a whole]], and [[a fragmented understanding of the portions of the domain they do learn]].

A fragmented understanding of the domain as a whole: A casual reader will not read all the works of a given domain, and so if information is fragmented into different disconnected works, the reader will come away with a fragmented understanding, only knowing about the points included in the particular work they read

A fragmented understanding of the portions of the domain they do learn: Even among the works a casual reader does read, they may fail to connect the information from the different works into a seamless network where every point is accessible by every other, producing a fragmented understanding where each point exists multiple times in different permutations, and a reference seen in one place doesn't end up pointing to the concept the author had in mind, producing vague and incomplete concepts.

Non-experts cannot reach mastery over even small units: Non-experts are prevented from reaching mastery over even small portions of a larger corpus, because if the information relevant to fully understanding the small portion might be distributed anywhere within the larger corpus, then only an expert who has the resources to assemble the entire corpus will be guaranteed to find it all.

Creating large discrepancies in knowledge: If only an expert who is able to learn everything about a domain can reach a nuanced understanding of any part of it, then the population will be divided into a few experts who know everything about a domain, and many people with no nuanced understanding of any part of it.


* Gaps: "Gaps" is the idea that [[when transmitting a graph via smaller linearizations]], [[as the corpus gets larger, the chances of having incomplete information at any given point grows]], and [[even for information that is highly valued]], and [[Canopy helps by presenting information exhaustively]].

When transmitting a graph via smaller linearizations: If a person builds an understanding for example by reading many books and articles on the subject.

As the corpus gets larger, the chances of having incomplete information at any given point grows: If the domain is very large, there is a good chance that there are points that didn't make it into any of the particular books or articles read by the person.

Even for information that is highly valued: Even if someone is very interested in a subject, there is a good chance that there is basic information about something they would be very interested in hearing about that they happen to have never heard because it just never was relevant to include in any of the narrow books and articles they've happened to see on the topic.

Canopy helps by presenting information exhaustively: In Canopy, a topic's paragraph is an exhaustive list of all information in the graph about that topic, and so it is much harder for pieces of information to fall between the cracks, as opposed to someone who is constructing an understanding by reading various books and articles, none of which may be attempting to provide an exhaustive list.


* Undersupply: Undersupply is the idea that [[there is a systematic undersupply of expert explanation]], and [[systems like Canopy offer good-enough substitutes for expert explanation]].

There is a systematic undersupply of expert explanation: For most students, access to individual tutoring from an expert would be highly beneficial, and the reason that this is impossible is cost and limited expert time, not an excess of student expertise, and so this is reason to believe that customized access to expert explanation would be beneficial to many people who cannot access it at present.

Systems like Canopy offer good-enough substitutes for expert explanation: At the present, [[expert time was the limiting factor to accessing expert knowledge]], and so [[mere access to expert knowledge will make a previously finite resource infinite]].

Expert time was the limiting factor to accessing expert knowledge: Currently, many people would have many more queries for experts than experts have time to answer.

Mere access to expert knowledge will make a previously finite resource infinite: If previously access to expert knowledge was limited, just making that information available, by using mediums more amenable to large interconnected knowledge sets, many people in the market for expert explanations might be able to settle for this "good enough" imitation of access to an expert.


[motivations/Observations]

* Interface of the expert: One of the design goals of Canopy is to mimic [[the interface of speaking with an expert]], as opposed to [[other forms of knowledge representation]], because [[information that is presented in a human format is more likely to be assimilated]].

The interface of speaking with an expert: If we [[store information the way an expert does]], then we should be able to [[reproduce the interactions a person can have with an expert]].

Store information the way an expert does: [[Experts store information in small units]], and [[experts store information in densely connected units]].

Experts store information in small units: From that an expert can give a very brief explanation of a subject, or reuse the same small explanation in multiple larger explanations, we see that the units of information they are accessing are small.

Experts store information in densely connected units: From that an expert might mention a fact in many different contexts demonstrates that they have a dense network of connections between the different facts that they know.

Reproduce the interactions a person can have with an expert: A system that stores information like an expert does should be able to [[accept follow-up questions]], [[reuse explanations]] and [[mention interconnections]].

Accept follow-up questions: An expert might say a brief description of an idea, and the listener is able to inquire into any part of it, producing more statements, which the listener can pose more questions to. Canopy models this by presenting brief statements and having room for follow-up questions to be anticipated and addressed.

Reuse explanations: If a system stores explanations in small reusable units like an expert does, then that system should be able to produce larger customized explanations out of those reusable components, like an expert can.

Mention interconnections: If a system stores the dense interconnections between facts like an expert does, then that system should be able to mention those interconnections any time they are relevant, like an expert can.

Other forms of knowledge representation: Other systems attempt to [[model a set of facts logically]], even if [[people don't organize information logically]], or they [[model a set of facts using linear prose]], even if [[people don't organize information as linear prose]].

Model a set of facts logically: Many systems of knowledge representation represent facts as a set of logical connections between entities, and so "A is related to B" and "B is related to A" are represented by the same connection between A and B, and you can't have one without the other.

People don't organize information logically: People do not organize knowledge as a simple set of relations between entities. For example, [[humans represent facts and their inferences separately]], and [[there is hierarchy to human fact storage]].

Humans represent facts and their inferences separately: [[A person can know a fact but not its inferences]], and this makes sense because [[non-inferential fact storage is useful]].

A person can know a fact but not its inferences: It seems possible for a person, when they think about A, to remember that "A is related to B", but when they think about B, to not remember that "B is related to A," which seems to indicate that these facts are stored in the mind separately, as opposed to being stored as a single bidirectional connection between entities.

Non-inferential fact storage is useful: It can be helpful to store facts without their inferences. For example, I might want to [[record my intention to do a task]] without [[adding it to my list of things to do]], but I can't do that with logical storage because [[storing facts and inferences together destroys information about directionality]].

Record my intention to do a task: When I think about going to the store, I might want to remember that it is something I'd like to do, but only when I was thinking about going to the store already.

Adding it to my list of things to do: On the other hand, I might want to put "going to the store" on my list of "things to do," such that I will think about it any time I think about things I want to do, which might be much more frequently than I think about going to the store independently.

Storing facts and inferences together destroys information about directionality: If my intention to go to the store is stored as a simple bidirectional relationship between "things I want to do" and "going to the store," then I cannot label "going to the store" as "something I want to do" without adding it to my master list of "things I want to do," resulting in an overly long and disorganized to-do list.

Model a set of facts using linear prose: Some systems try to represent human knowledge as a series of articles on different subjects, with each article representing what a person might know about that subject.

People don't organize information as linear prose: Linear prose fails to capture many qualities of human knowledge, for example, that [[people store information as large collections of small units]] whereas [[linear prose stores knowledge as small collections of large units]].

People store information as large collections of small units: [[A person stores information about a subject as small units]], but [[a person can store a large number of units]].

A person stores information about a subject as small units: When you ask an expert for an explanation of a subject, they can give you a very brief explanation, and can then choose to expand on it, indicating that the units they actually store the information in are small.

A person can store a large number of units: Even the units of a person's knowledge are small, a person can string together immense structures of information build out of these small pieces.

Linear prose stores knowledge as small collections of large units: [[Linear prose stores information in large units]], but [[the total size of linear prose is limited]].

Linear prose stores information in large units: The typical article is much longer than anything a person would say from memory, indicating that prose is stored in much larger units than human memory is.

The total size of linear prose is limited: Even though the units of prose are large, the total amount of information that can be stored in a book or article is fixed, unlike human memory which has no obvious limit.

There is hierarchy to human fact storage: [[People segment relations by scale]], but [[scale information is lost in logical fact storage]].

People segment relations by scale: If a person wants to memorize the cities of a country, they might memorize a list of the top three or four, and then store the rest by region, eg "cities of the American Northeast." Thus, they would still be able to derive the full list, but they have organized the information by magnitude so that more important examples come to mind before less important ones.

Scale information is lost in logical fact storage: If the cities of a country are stored as simple bidirectional relationships between the the city and the country, then there is no simple way to label the city as belonging to the country without storing the city on the list of the country's cities, resulting in a long undifferentiated list of cities where less significant examples come to mind as frequently as more significant ones.

Information that is presented in a human format is more likely to be assimilated: There are many projects that allow a user to navigate a network of information, but their goal is not to produce _the_ output that an expert would. Experts producing explanation are performing a traversal of information stored in "the human knowledge format," whatever that is, and information produced from the human knowledge format is more likely to be directly assimilable by listeners _into_ the human knowledge format, and so by trying to preserve that format, Canopy presents information in the way most likely to be mentally reconstructed by readers into the desired understanding.


* Realism: Realism is the idea that [[human explanation really is the serialization of a graph]], and so [[any representation of explanation besides a graph will be distortive]], which is why [[Canopy represents the underlying explanation graph]], so that [[we preserve all possible explanations that can be produced from the original data]].

Human explanation really is the serialization of a graph: [[Humans explain things in serial]], so [[you might think that explanation is inherently serial]], but actually [[there are many valid serializations of a knowledge set]], and so we see that [[serializations are just one of many presentations of an underlying data structure]], and [[it makes sense to think that the underlying data structure is a graph]].

Humans explain things in serial: A book or lecture is inherently a list of statements, one after another, in a straight line. It has to be this way because speakers can only say one thing at a time and listeners can only listen to one thing at a time.

You might think that explanation is inherently serial: Because all of the explanations we see are linear, one thing after another, you might think that explanations are inherently linear, and that all an explanation is is a list of sentences.

There are many valid serializations of a knowledge set: We see that an expert can express the same knowledge in multiple traversals, for example when an expert writes multiple books and articles on the same topic but in different orderings and levels of detail.

Serializations are just one of many presentations of an underlying data structure: Because we see the same knowledge can be used to produce many different explanations, we see that linear explanations are not a true representation of the underling data, but are rather ephemeral permutations being generated on-the-fly from a more fundamental structure.

It makes sense to think that the underlying data structure is a graph: A graph is composed of a series of entities and connections, and so if knowledge were stored as a graph, it would make sense how you could make many different explanations from the same original data, because with a single graph you can explore the nodes via their connections in many different orderings.

Any representation of explanation besides a graph will be distortive: If you represent a graph as a linearization, either you will go deep into detail before going broad, making it hard to get a high-level picture, or you will go broad before going deep, making it hard to dive into details of a subject.

Canopy represents the underlying explanation graph: Rather than have an author produce a series of books and articles, capturing their knowledge graph only approximately, Canopy has the author capture the underlying graph itself, offering tools to make this easier for them to do.

We preserve all possible explanations that can be produced from the original data: [[The author in person can produce many linearizations]], but [[capturing their knowledge as a sequence destroys this original flexibility]], however, [[if the author captures their knowledge with a graph]], [[we preserve the original multiplicity of possible explanations]].

The author in person can produce many linearizations: An expert in-person can produce many different linearizations of their knowledge for different people.

Capturing their knowledge as a sequence destroys this original flexibility: If the expert captures their knowledge as one big book, then we went from having the expert, who could produce infinitely many linearizations of their knowledge, to having a book, which is only one linearization, destroying all of the other potential explanations the expert could have produced.

If the author captures their knowledge with a graph: If the expert uses a tool like Canopy to capture their underling knowledge graph as opposed to writing a single long book,

We preserve the original multiplicity of possible explanations: From the graph that the expert produces, we are able to produce the infinite number of possible linearizations that the expert themselves could produce, meaning that less creative potential is lost in converting expert knowledge to a graph format than is destroyed when converting expert knowledge into linear text.


The author can produce many possible linearizations, but if the author produces a linearization, we only captures their underlying graph rather than capturing only a few particular traversals like books or articles, then we have richer data from which we can can generate all the possible linearizations of the graph, allowing one reader to go into depth whereas another reader can get a general picture, a potential that would have been lost if the author had just produced one big text.


[motivations/Possibilities]

* Completeness: Completeness is the idea that once an author is able to produce works of larger [[scale]], it will be possible to achieve completeness of analysis, such as [[recursive consideration of arguments]], so [[non-inclusion will be clearly intentional]] and [[the costs of including objectionable content will be lower]].

Recursive consideration of arguments: Sometimes an author will give an argument for a view, and maybe also show a problem with the argument, but in theory, the tree of arguments and problems with the arguments, and problems with the problems, etc, can become very large, and it is difficult to achieve such coverage of even a small subject area.

Non-inclusion will be clearly intentional: At present, if an author doesn't include a given point or argument, they can claim it is because there isn't space to consider every view. With [[non-rival media]] however, when the author doesn't include a point, it will be more clearly a choice, which might motivate authors to present wider coverage.

The costs of including objectionable content will be lower: Many are skeptical of including coverage of arguments they disagree with for fear a reader will see the argument and not the rebuttal, and walk away with an incorrect view, but in Canopy (which has [[informed abridgement]]), the same paragraph that mentions a view will include mention of its rebuttal, and so the reader can only come away with an incomplete view if they themselves choose to, making it less risky for authors to include discussion of objectionable views.


* Explanatory Invariants: Explanatory invariants is the idea that [[there are reoccuring types of relation between facts in a corpus]], and that [[it is sometimes desirable to capture every relation of a given type]], and while [[free-form prose makes it difficult to capture explanatory invariants]], [[Canopy makes it easy to capture them]], and [[without edging out existing analysis]].

There are reoccuring types of relation between facts in a corpus: You might have a corpus of historical facts and for every fact claim one could ask "what historical documents exist to corroborate this fact?" Or, you might have a description of the components of a system, and for each one you might ask "how does this component work?" Or for every idea stated in a discussion you can ask, "what is the historical origin of this idea?"  Thus, a corpus can be composed of reoccuring types of fact, and each type of fact might always have a certain related question.

It is sometimes desirable to capture every relation of a given type: If we want to have [[completeness]] in capturing a corpus, it might be desirable to express the expectation that every fact of a certain type should have a relation of a certain type.

Free-form prose makes it difficult to capture explanatory invariants: When writing books and articles in free-form prose, different questions and forms of analysis are applied ad-hoc, and it can be difficult to see whether a certain question is asked uniformly in every place where it is relevant.

Canopy makes it easy to capture them: In Canopy, by modeling topics and all their relations in an abstract way before filling in the details, it is easier to see whether a certain question or relation is being expressed for every statement of a given type, and writers can make templates that require topics of a certain type to have different expected subcomponents, for example every historical event having a date or corroborating documents.

Without edging out existing analysis: If you take a book about different historical events and add a list of historical documents corroborating every point, then these added sources are making it harder for people to get the flow of the events themselves. Yet with Canopy, every point of type "historical claim" can have a subpoint of type "corroborating documentation" without these subpoints taking space away from the original analysis, which is an example of how Canopy is [[non-rival media]].


* Holistic Review: Holistic review is the idea that rather than [[reviewing information in small pieces]], [[Canopy allows people to review information in large connected units]], which is possible because [[Canopy allows progressive occlusion of referenced prerequisites]] and because [[Canopy allows repeated access to complex structures]].

Reviewing information in small pieces: When someone makes flash cards to learn an interconnected subject matter, they are reviewing pieces of that subject matter independently without also reviewing the connections at the same time.

Canopy allows people to review information in large connected units: By iterating repeatedly over a Canopy graph in different traversals, a reader is able to review not only the facts but also an organizing structure that includes the facts and relates them to one another.

Canopy allows progressive occlusion of referenced prerequisites: While [[a first time reader only can cover small portions of graph in each session]], [[over multiple reviews the size that can be reviewed in each session increases]], and because [[references reviewed in separate sessions can become fragmented]], [[larger review sessions produce better understanding]].

A first time reader only can cover small portions of graph in each session: When a person reads a Canopy paragraph for the first time, they might have to open many child paragraphs to understand all the different prerequisite concepts, so they end up not covering that much material because they need to burrow into so much clarification.

Over multiple reviews the size that can be reviewed in each session increases: As the reader reviews more and more, eventually they will remember the definitions of various terms and thus will not have to open as many child paragraphs, and so they can review larger and larger portions of the graph in one sitting.

References reviewed in separate sessions can become fragmented: If someone learns about New York on Sunday and hears that its capital is Albany, and then on Friday learns about New Jersey and hears that it is next to another state called New York, the person might fail to connect that the New York they heard about on Sunday is the same one they later heard about on Friday, and so they would fail to understand that New Jersey is next to a state whose capital is Albany, even though they've heard everything they need to understand it.

Larger review sessions produce better understanding: If a reader sees a description of something and then sees it referenced on the same day, there is a much better chance that this will produce a seamless, unfragmented understanding where new knowledge added to any point becomes accessible from every other.

Canopy allows repeated access to complex structures: When learning a complex new domain, one inherent difficulty is that you can't create interconnected knowledge without review of the various parts and their connections, but on the other hand, it is difficult to review a complex interconnected domain without continued access to the instructor because you lack a master copy of the interconnected knowledge you're trying to internalize, (and [[review of a complex system in stages can be problematic]].) So, Canopy allows the instructor to make available a perfect copy of the "goal" understanding, so that students have a reliable version to review in the first place.

Review of a complex system in stages can be problematic: Why not just have the student take notes on the basics, review that, and over time build their understanding of the system as a whole? It is possible, but one difficulty is that an oversimple initial understanding can stick long after it is time to have understood better, and so if the student begins by reviewing notes on a few disconnected incomplete statements, it will be much harder to later add the missing information than if from the get-go they had reviewed definitions that mentioned the existence of further complexity and interrelation so that later they could add that nuanced information without fundamentally changing their initial understanding.


* Interconnection: Interconnection is the idea that [[the connections between different facts are valuable data]], but that [[linear content limits the representation of interconnections]], whereas [[Canopy decouples information storage from presentation]], and so [[Canopy increases the number of connections that can be stored]], making it more similar to the [[interface of the expert]]. Interconnection is the opposite of [[fragmentation]].

The connections between different facts are valuable data: Even if a person knows many facts, if they lack an understanding of the connection between them, they will miss implications from one to another, and new information might not update all relevant knowledge.

Linear content limits the representation of interconnections: When information is stored in linear books and articles, the author cannot share every relevant interconnection because [[audience interest is limited]] and [[the shared prior knowledge is small]].

Audience interest is limited: The audience might not be interested in hearing every connection between every idea, and so the author won't include them even for those readers who do.

The shared prior knowledge is small: The author can only include interconnections between facts that the readers know about, which limits the author to interconnections between facts mentioned in the given book or article, but because [[there are constraints on the size of books and articles]], this ends up meaning that [[there is no work that stores enough facts to also include all interconnections]].

There are constraints on the size of books and articles: Practically speaking a book or article can only become so long and retain reader interest.

There is no work that stores enough facts to also include all interconnections: No work has all the given points, and no work can connect points that it doesn't include, and so no work exists that can include all the interconnections of a domain, even if there would be reader interest.

Canopy decouples information storage from presentation: With Canopy, what the author writes and what the reader reads doesn't have to be the same thing.

Canopy increases the number of connections that can be stored: The author can store all the points and interconnections they know about, and not every reader has to read every one.


* Pervasive Concepts: Pervasive concepts is the idea that [[within a domain, there are certain pervasive concepts]], and [[it can be difficult for the reader to keep track of all the situations in which they've seen a pervasive concept]], and [[keeping track of different examples is be necessary to produce an abstract understanding of a pervasive concept]], and so [[Canopy helps by allowing you to link to pervasive concepts as you work through the material of a course]].

Within a domain, there are certain pervasive concepts: In programming for example, you have the concept of modularity, that large systems can be composed of smaller units that hide their internal structure in order to make things more understandable. This idea can be found in different presentations and permutations throughout the branches of engineering.

It can be difficult for the reader to keep track of all the situations in which they've seen a pervasive concept: Throughout a computer science education, a student might have seen a concept like modularity crop up in hundreds of situations, and if they were a more casual reader, they might have forgotten some of them.

Keeping track of different examples is be necessary to produce an abstract understanding of a pervasive concept: In order to have an abstract and flexible concept of modularity, it might be necessary to remember many different examples, and hold in mind at the same time the way in which each example has unique details but still expresses the concept abstractly.

Canopy helps by allowing you to link to pervasive concepts as you work through the material of a course: By using [[redundant indexing]], all the units of a curriculum can be accessible by topic, but also accessible by theme, and so a reader can review prior material specifically in so far as it illustrates a pervasive concept like modularity.


* Prescriptive Understandings: Prescriptive understandings is the idea that [[experts have vast organizational structures]], but that [[experts omit portions of their mental representation when speaking]], and [[this prevents listeners from building the proper understanding]] but [[Canopy requires the author to enumerate the desired understanding]].

Experts have vast organizational structures: Experts don't just memorize a list of facts, they have many heuristic subcategories that are a function of their way of grouping things.

Experts omit portions of their mental representation when speaking: Sometimes an expert's private groupings are extremely useful practically to have, yet aspects of them may be controversial or oversimple, and so the expert might be motivated to omit them and just present the facts in sequence without offering any helpful mnemonics or groupings.

This prevents listeners from building the proper understanding: The expert is only able to navigate their knowledge using these helpful groupings, and so by removing them from the explanation, the expert is failing to fully enumerate the mental structure that the listener should produce from the data.

Canopy requires the author to enumerate the desired understanding: Canopy requires information be presented hierarchically, and so it isn't possible to give undifferentiated lists, and so authors will be forced to provide the reader with helpful intermediary groupings with which to retrieve the information in question.


* Redundant Indexing: Redundant indexing is the idea that [[the different entities of a subject area can be organized in different ways]], and [[Canopy lets you explore the data of a project in these different organizations]].

The different entities of a subject area can be organized in different ways: You might want to access a list of diseases by the part of the body they affect, but you might also want to practice recalling them based on symptoms, so we want to access the same entities with all their optional detail, but via different organizational schemes.

Canopy lets you explore the data of a project in these different organizations: Canopy lets you define a set of topics with all their details and subcomponents, but also to define arbitrarily many indexes of these topics, listing different groups of topics in different ways, and no matter what manner you arrive at a given topic, you can still burrow into the details and remind yourself of the specifics of the topic before trying to redundantly index it via multiple organizational schemes.


* Rich Compositions: Rich compositions is the idea that [[descriptions can be complex]], but [[humans are currently limited in the size of the descriptions they can consume]], and so because [[Canopy makes it easier to share rich compositions]], [[Canopy increases the maximum complexity of ideas that can be successfully communicated]], creating the potential for a world where [[complex ideas are communicated with higher fidelity]].

Descriptions can be complex: A description of a complex thing might require describing several aspects of the thing, which might in turn require defining several terms, which might require giving background information, and so the overall description could end up being a complex structure of many small nested explanations.

Humans are currently limited in the size of the descriptions they can consume: There are [[limits to working memory size]], and there are [[limits to maximum session length]].

Limits to working memory size: A person can only keep track of a certain number of things at any given time, so a definition that contains a definition that contains a definition might end up overflowing the amount of things the person can keep track of, causing them to forget how all the parts fit together into the original big picture.

Limits to maximum session length: The amount of content that can be communicated in a given session is limited by the listener's attention span, and information communicated in one session might be forgotten by the next, making it difficult to keep all the required subdefinitions in memory long enough to form the desired picture.

Canopy makes it easier to share rich compositions: Canopy makes it easier to make rich compositions because [[Canopy makes it easier for readers to reconstruct complex compositions]], [[reusability makes it easier for authors to produce complex compositions]], and [[Canopy removes traditional limits on content length]].

Canopy makes it easier for readers to reconstruct complex compositions: By showing a brief paragraph, followed by a definition, followed by a definition required for the definition, and so on, a reader can burrow many levels down in the composition of an idea, and yet still still see clearly on the page how they got to a given point, and afterwords can regress back upwards and progressively reassemble the pieces they've seen into the overarching original composition, all without having themselves to keep track of the complexity of what piece goes where.

Canopy removes traditional limits on content length: Whereas books and articles have a fixed maximum length, preventing authors from sharing many rich compositions due to length constraints, Canopy projects decouple storage from presentation, and so can grow to a very large [[scale]].

Canopy increases the maximum complexity of ideas that can be successfully communicated: If it becomes easier for authors to communicate to readers complex ideas that require the proper assembly of many complex subcomponents, and if readers become more likely to be successful in reconstructing such complex compositions, then authors and readers might choose to express and consume ideas of greater complexity than are currently communicated.

Reusability makes it easier for authors to produce complex compositions: Canopy allows authors to produce [[reusability|reusable units of explanation]], and thus it is easier for authors to produce complex compositions because they can reuse terms and ideas that they've defined elsewhere.

Complex ideas are communicated with higher fidelity: The friction with which complex ideas are communicated is one of the major limiting factors of growth and economic mobility in the world today. Access to videos and textbooks is high, but career paths like medicine and engineering still are extremely challenging for most people due to the difficulty entailed in converting these resources to a nuanced understanding, so any medium that reduced this friction could benefit many people.


* Scale: Scale is the idea that [[linear mediums limit the maximum size of explanations]], whereas [[non-linear media allow greater explanation size]].

Non-linear media allow greater explanation size: Non-linear media can grow larger than linear media because [[non-linear media is more navigable at large sizes]], and also [[non-linear media isn't constrained by audience interest]].

Non-linear media is more navigable at large sizes: [[Difficulty navigating large corpuses limits corpus size]], so because [[non-linear organization increases the ease of navigating large corpuses]], [[non-linear media enables larger explanations]]. (But [[what about search?]])

What about search? Doesn't the existence of search make it tractable to find things in a large corpus of linear explanations very quickly? Yes, but [[identifying the right search terms and content tagging is difficult]], and [[search doesn't give the reader an organizational scheme]].

Identifying the right search terms and content tagging is difficult: If I am searching for content, I have to predict what keywords it will include, or the author will have to have labeled it properly, and in large corpuses, anticipating how to label content and search for it becomes more difficult.

Search doesn't give the reader an organizational scheme: When the reader accesses the portions of a corpus by search, they are jumping directly to one piece of information, and don't know anything about where it is in the grand scheme of things, whereas if they access the information via a subsuming tree structure, they will have a general idea of what exists, and can use that structure to organize the information in their mind.

Non-linear media isn't constrained by audience interest: Non-linear media can grow large even if people prefer to consume small explanations because [[non-linear explanations decouple storage from presentation]], and so [[demand for content of a certain size won't prevent large corpuses]].

Linear mediums limit the maximum size of explanations: Authors are constrained in the length of books and articles by [[medium constraints]] and [[audience relevance]], and so even if the author has information they would like to publish, often they have no good place to put it. (And [[even niche works have the same problem]].)

Medium constraints: Practically speaking, a book or article cannot become greater than a certain size, both because there isn't an audience for such large works, and sometimes because of literal constraints on how much content can be printed and circulated.

Audience relevance: Given that most readers of a book or article are reading more or less from beginning to end, an author can't include things that aren't relevant to all readers.

Even niche works have the same problem: You might say [[niche content solves limitations of audience relevance]], but [[niche content is constrained by audience relevance on a smaller scale]], and [[fragmented works create problems of discoverability]].

Niche content solves limitations of audience relevance: You might say when there is a subpopulation that wants more information than the rest of the audience, an author can produce additional chapters, books, or articles just for them, meaning there will always be space for any given piece of information.

Niche content is constrained by audience relevance on a smaller scale: Niche books and articles will also be forced to leave things out, because within the niche readership there will be people with different particular interests, and the author still won't be able to include points that aren't relevant to all readers.

Fragmented works create problems of discoverability: Even if an author could find space for everything they want to share by producing multiple works for different audiences, this produces problems of [[discoverability]] because eventually not everyone will know about the existence of all the various works and what they contain, defeating the purpose of finding a place for information so that it can be shared.

Difficulty navigating large corpuses limits corpus size: Even if we are physically able to store large corpuses of information, if we don't have a good way of navigating them, then people aren't going to go to the trouble to assemble that much information in the first place.

Non-linear organization increases the ease of navigating large corpuses: One of the foundational findings of computer science is that [[finding things in a list is slow]], but [[finding things in a tree is quick]], and so if non-linear media organizes content by tree, it will drastically increase the speed at which the reader can find what they are looking for.

Non-linear media enables larger explanations: Because it is more easily navigable at large sizes, non-linear media removes one of the limiting factors on the size of explanations.

Finding things in a list is slow: Finding something in a linear sequence like a book requires scanning the entire sequence, and even tables of contents require scanning the table of contents, which becomes impracticable if there are many things in it.

Finding things in a tree is quick: Imagine an employee is trying to find their name in a company org-chart. All they have to do is find their division, department, and team. Thus, they are able to find their name while only looking at a few nodes in the tree, which is much faster than having to read through a list of every employee name. This is an example of how finding things in trees is faster than finding them in lists.

If tree organization makes it easier for users to find what they are looking for in large corpuses of information, then it will be worth while for authors to produce larger corpuses.

non-linear explanations decouple storage from presentation: With customized explanations, what the author produces and what the reader consumes don't have to be the same thing, and so the author is no longer constrained to produce content in exactly the format in which the reader wants to read it.

Demand for content of a certain size won't prevent large corpuses: Even if readers want explanations of a certain size, authors will not be constrained to produce only that much content, but can produce much more, and let each reader read what they want to.


[motivations/Qualities]

* Clarity: Clarity is the idea that [[explanations can contain extra information that assists the listener in reconstructing the expert's understanding]], and [[Canopy forces authors to include this information]].

Explanations can contain extra information that assists the listener in reconstructing the expert's understanding: [[Explanation has a nested structure]], but [[the connections between parts of an explanation can be unclear]], and so [[elaborate verbalization can help the listener parse the explanation]].

Explanation has a nested structure: Explanation is a description of something, and it is a description that uses terms which sometimes must themselves be explained, and sometimes those explanations include terms that need explanation, so at any given point, the explainer might be explaining a term in order to use it in the explanation of a term, and so on.

The connections between parts of an explanation can be unclear: It is sometimes unclear to the audience how the thing the author is currently speaking about relates to the original topic. For example, a story which is now being told might be intended to illustrate the meaning of a term which the speaker wants to use to describe the original topic, but the author might not have said so explicitly because it seemed obvious. If this remains unclear to the listeners, they will end up with a fragmented understanding, remembering the story itself but not connecting it to their understanding of the original topic.

Elaborate verbalization can help the listener parse the explanation: The author can help the listeners follow the flow of the explanation by adding redundant explicit explanation of how each step of the explanation relates to the previous one. The author might say "I want to explain a certain historical event, but first, in order to understand why it happened, I am going to explain a principle of economics. But, in order to understand this principle of economics, I need to tell you a story that illustrates it," and so on. Thus, every piece of the explanation is clearly related to the piece that came before it.

Canopy forces authors to include this information: In Canopy, every idea is expressed in a paragraph, and the only way to get from one paragraph to another is if the first paragraph mentions a concept that is explained by another. Thus, it is physically impossible for an author to leave it ambiguous why they are discussing a certain point, because by definition, the only way to access a point is by tracing the path to it from the original topic. Thus authors are forced to include this clarifying information even when it seems clear to them already, helping those readers for whom it wasn't clear.


* Connected Synonyms: Connected synonyms is the idea that explanations often [[include unclear synonyms]], which [[produces a fragmented understanding]], and Canopy helps by [[interlinking synonyms]].

Include unclear synonyms: Sometimes an expert will use one term for something, and sometimes another, and this could be intentional, in order to refer to slightly different aspects of that thing.

Produces a fragmented understanding: If the listener does not know that these synonyms are referring to the same thing, they might come away with the impression that there are two different things, and that they haven't gotten the full story about either one of them, an example of the phenomenon of [[fragmentation]].

Interlinking synonyms: In Canopy, the author can represent every synonym as its own entity, and then use natural language explanation to connect the different synonyms, so that no matter which term they use, the reader can burrow into the reference to connect it with all the related terms.


* Customization: Customization is the idea that whereas [[a human explainer can customize explanations for a given listener]], a book, by contrast, is [[mostly the same experience for every reader]], and while [[linear text is efficient for homogeneous groups]], [[Canopy aims to provide the type of customization a human explainer can provide]].

A human explainer can customize explanations for a given listener: A human explainer can follow the listener's particular interests, adding more detail for someone advanced, or more background information for a beginner.

Mostly the same experience for every reader: A reader can flip around and read some chapters and not others, but fundamentally they are consuming long fixed blocks of linear content, unlike spoken explanation where the units are small, and the direction of the explanation can change based on feedback from the listener.

Linear text is efficient for homogeneous groups: Sometimes you have a group of people who want an explanation, and due to the circumstances, [[their needs are very similar]]. In these cases, [[it is more efficient to use linear prose than it is to use Canopy]].

Their needs are very similar: Two examples of groups of people with similar needs are [[standardized test takers]] and [[news readers]].

Standardized test takers: If students are taking a standardized test, and it is testing them on a subject area that is new to all of them, and the test requires the same level of knowledge for everbody, perhaps their background knowledge is identical, and their desired knowledge is identical, in which case they might not benefit from having customized explanations. (However, even in such a case it might be helpful to have [[customization in review]].)

Customization in review: When students are reviewing information, even if they all are aiming for the same level of final understanding, each particular student might benefit from reviewing different aspects of the content based on where they are having difficulties.

News readers: When an event occurs in the news and there isn't very much information available, most people are in the market for the same explanation, because that's all that is available. However, [[some news stories might benefit from customization]].

Some news stories might benefit from customization: [[Some news stories are complex]], and [[some news stories relate to background knowledge]], therefore some news stories do benefit from customization.

Some news stories are complex: When a news event involves an ongoing sequence of interrelated events, some readers might benefit a recap of the prior context, whereas others don't need it and would be annoyed by it.

Some news stories relate to background knowledge: If a news event has historical context or benefits from theoretical background knowledge, some readers may be interested to learn more about the related background knowledge, and some might just want the practical facts.

It is more efficient to use linear prose than it is to use Canopy: If a group of people all need the same explanation, writing it in linear text is more efficient than writing it in reusable blocks because [[the author can make better assumptions about the prior knowledge of the readers]], and [[the author can more easily capture learning requirements]].

The author can make better assumptions about the prior knowledge of the readers: [[An author of a book or article knows the context of the reader's session]], and [[a customized explanation does not]].

The author can more easily capture learning requirements: If the user is allowed to explore content freely, they might not learn everything they are responsible for knowing, whereas linear text can force them to go through a certain set of content.

An author of a book or article knows the context of the reader's session: An author of a book or article knows what the readers have read so far and what they are going to read, and so the author can reference things they know the reader has seen already, or avoid a certain subject for now because they know it will be explained later at a better time.

A customized explanation does not: Customized explanations are produced by combining reusable units that were not written with knowledge of what the reader has seen or will see, and so customized explanations might accidentally offer the reader information they already know from earlier in the session, or offer information before it is optimal for the reader to learn it.

Canopy aims to provide the type of customization a human explainer can provide: Canopy aims to provide customized explanations to each reader, allowing them to follow their specific interests and request additional information or clarification. ([[But isn't human explanation too complex to reproduce with digital tools?]])

But isn't human explanation too complex to reproduce with digital tools? One of the theses of Canopy is that [[explanation composition is complex]], but [[explanation assembly is not complex]], and [[Canopy only models explanation assembly]].

Explanation composition is complex: When a human composes a new explanation, they are drawing on the sum of their life experience and linguistic abilities, and this is an extremely complex process. In order for a computer to duplicate this task well, it would have to have similar breadth of knowledge to a human, which would be very difficult to accomplish.

Explanation assembly is not complex: The hope of Canopy is that if an author produces explanation in reusable blocks, the assembly of these blocks to fit the needs of the listener is a task that can be automated and done without requiring further input from the expert.

Canopy only models explanation assembly: The purpose of Canopy is to have the author express their knowledge in reusable units that the front-end client can combine into new explanations. Therefore, Canopy is not attempting to replace the author or the process of explanation composition, but only the latter stage of explanation assembly.


* Discoverability: Discoverability is the idea that [[it should be easy to find out what information exists in general]], and that [[it should be easy to find information related to a given point]]. Canopy attempts to [[make discovering the scope of available information easier]], and to [[make discovering related points easier]].

It should be easy to find out what information exists in general: Often it is very difficult even to just find out what information exists in a domain so that one can know what to request.

It should be easy to find information related to a given point: When someone sees a given piece of information, they should be made aware of other related information, which requires the media to represent the interconnections between the different pieces.

Make discovering the scope of available information easier: By [[decoupling detail from scope]], Canopy makes it easier to get a bird's-eye view of the corpus in order to inform one's requests for information.

Make discovering related points easier: By representing the [[interconnection]] between points, Canopy lets the user discover all the information related to what they're reading.

Decoupling detail from scope: Whereas linear prose will contain a mix of new points and details, in Canopy, details are displayed separately from new points, making it easier to see the scope of available content without getting bogged down in all the available details.


* Exhaustiveness: Exhaustiveness is the idea that whereas linear prose is [[written additively]], in Canopy text is [[written exhaustively]], and this [[simplifies learning]].

Written additively: In prose, the author might say "there are cats and there are dogs," but I have no way of knowing if these are all the kinds of animal, or whether there will be other ones scattered throughout the work.

Written exhaustively: In Canopy, you would have a paragraph called "kinds of animal" and it would exhaustively list all the kinds of animal, because every category or entity has one dedicated paragraph in Canopy which must exhaustively contain all information on that topic.

Simplifies learning: If paragraphs are not written cumulatively, students might end up learning incomplete lists in multiple places, causing [[fragmentation]], whereas exhaustive descriptions put everything in one place, and are more likely to end up together in the mind of the reader.


* Explicit Prerequisites: [[A linear work represents prerequisites implicitly via sequence]], [[making it difficult to consume small parts of a large work]], but [[Canopy represents scope explicitly via prerequisite enumeration]], [[making it easier to consume parts of a large work]].

A linear work represents prerequisites implicitly via sequence: In a book or lecture, the author assumes the listener knows everything that was said earlier in the session. Thus, the set prior knowledge that is assumed and might get referenced in a given paragraph is represented approximately by the set of all earlier paragraphs, even if not all prior paragraphs are actually necessary to understand a given point.

Making it difficult to consume small parts of a large work: If every paragraph could in theory reference ideas developed in any prior paragraph, then there isn't a good way to read just one paragraph of a book because you don't know which specific prior paragraphs it is building one.

Canopy represents scope explicitly via prerequisite enumeration: Canopy represents the prerequisites of each idea explicitly, and so you can in effect "read one paragraph from a book," and only those earlier paragraphs that are necessary to understand it. So, the "scope" of prior knowledge that the author is able to reference is represented explicitly via linked prerequisites, as opposed to implicitly via the sequence of the linear work.

Making it easier to consume parts of a large work: If the scope of each paragraph is defined explicitly, by labeling every prior idea that is mentioned explicitly, then it is very easy to pick a paragraph and read only those paragraphs that are necessary to understand it.


* Hierarchy: Hierarchy is the idea that [[in every complex domain, human understanding requires the creation of hierarchy]], and [[Canopy bakes hierarchy into the medium itself]].

In every complex domain, human understanding requires the creation of hierarchy: Because there are [[several limitations on the amount of information that can be dealt with at any given time]], [[humans deal with complexity by creating hierarchical models]], which [[enable them to manipulate complex information in small units]].

Several limitations on the amount of information that can be dealt with at any given time: Two such limitations are the fact that working memory can only store ideas composed of a certain number of "chunks", and that long-term memory can only directly associate a given fact or entity with a small number of other facts and entities.

Humans deal with complexity by creating hierarchical models: If a system has many interconnected parts, those who try to understand it might break it up into parts, or organize the interconnections between the pieces into categories so that different layers of the system can be studied one at a time.

Enable them to manipulate complex information in small units: By having ways of breaking a complex reality into simpler theoretical units, people are able to build models that are highly complex, without the author ever having to think about all the complexity at any single moment. By grouping the entities of the domain into larger hierarchical chunks, more information can be held in working memory at any given time, and by segmenting the connections different entities have by type, one can have good visibility into a single aspect of the network at each time.

Canopy bakes hierarchy into the medium itself: Canopy gives an author the ability to express a complex system out of a hierarchy of simpler parts, which makes it a natural tool for expressing the connections of a highly complex subject domain.


* Informed Abridgement: Informed abridgement is the idea that whereas [[other mediums abridge without informing the reader]], [[in a Canopy project the reader makes an informed decision about when to stop reading]], and thus [[the reader has a better sense of their level of knowledge]], [[reducing unknown unknowns]].

Other mediums abridge without informing the reader: When an expert writes in a popular publication, they will often simplify concepts and leave out opposing views due to the limitations of the venue, all without informing the reader of this, and so the reader doesn't know how much they don't know about the subject.

In a Canopy project the reader makes an informed decision about when to stop reading: In a Canopy project, an expert can include all relevant information on a subject, and so the reader can begin with a brief description that mentions the existence of more information, and if they request that information, it will mention the existence of still more information, and so at each level of detail, the reader is notified of what else there is to know, so that if they decide to stop reading, it is with full knowledge that they haven't seen everything there was to read.

The reader has a better sense of their level of knowledge: If the reader is offered more information and makes an informed decision to stop reading, then they will be much more modest in their self-appraisal because they know they are missing information, not like the reader of a popular publication.

Reducing unknown unknowns: When mediums abridge the complexity of a topic without notifying the reader, it is common for the reader to come away with an inflated sense of the accuracy of their understanding, because their mental theory accounts for 100% of the things they've heard about the phenomenon in question, but only because they know so little about the phenomenon.


* Memorability: [[Memory entails associating a cue with a response]] which is achieved by [[seeing the cue alone before the response]] but [[traditional prose presents information as a single unit]], so [[Canopy presents information in stages]], and [[this will make content easier to remember]].

Memory entails associating a cue with a response: It seems that the major systems of memory create associations so that one particular thought brings to mind another, and so when we think about teaching students a certain corpus, we have to be clear about what specific cues we want them to recognize, and what specific responses we want them to think of when they see those cues.

Seeing the cue alone before the response: In order to make someone remember to buy milk when they pass the grocery store, they should think about "passing the grocery store" by itself, and then think about getting milk, so that when they are in the real world and are passing the grocery store, the idea of passing the store by itself brings the idea of getting milk to mind. If they merely review the idea of "getting milk when passing the grocery store" as one big unit, then they are associating the task in memory with itself, and so the situation of "passing the store" by itself might not bring it to mind, defeating the purpose of memorization.

Traditional prose presents information as a single unit: When I learn information about New York, I want that information to be the "response" that comes to mind when I think about the "cue" of New York. Prose, however, presents me the cue and the response all as one unit, and so I am not practicing retrieval of the content by topic name, but rather, I am practicing retrieval of the content by content, which isn't helpful.

Canopy presents information in stages: Canopy presents a brief paragraph that mentions a few entities by name, and only once the user interacts with a topic name does the user see more information, so the "cue" of the topic name is seen before the "response" of the topic's paragraph.

This will make content easier to remember: In Canopy, the reader sees the "cue" of the link before the "response" of the child paragraph, and so the reader is being trained to expect the paragraph when they see the topic name alone, creating a vast network of cues that trigger responses which contain in them multiple further cues, producing mental structures that will enable the reader to reliably retrieve later the new information they are receiving, preventing "orphans" where information is learned but without creating a cue by which to access it later.


* Mergability: Mergability is the idea that whereas [[linear works address content approximately]], [[Canopy addresses ideas individually]], [[it is easier to merge multiple explanations]], [[making possible new types of composite works]].

Linear works address content approximately: In a book, ideas are organized by chapter and page number, and so the system of addressing isn't very precise.

Canopy addresses ideas individually: Every idea in Canopy has a unique address, and it exists at a point in a larger series of topics.

It is easier to merge multiple explanations: If two books speak about the same subjects, it is not very easy to "put them into conversation" and line up the bits that speak about the same things. With Canopy however, because the content is already organized in small labeled units, it might be much easier to line up one work with another and compare what they both say on a given topic.

Making possible new types of composite works: By making it easy to "merge" different works together, it might become possible to produce new types of work that cull all the points of a set of works by topic and present them in dialog, capturing every point made in the original works, but without following the exact train of thought of each original author, allowing readers to take a complex topic, break it into smaller questions, and see the range of perspectives that exist on each one. Thus, a group of people could produce a "balanced" presentation of views on all the points of a topic area even if they do not agree, so long as they agree on what question each view is coming to answer.


* Non-rival media: Non-rival media is the idea that [[linear mediums are rival]], whereas [[non-linear mediums like Canopy are non-rival]], and [[non-rival mediums have various benefits]].

Linear mediums are rival: In books and articles, including one point comes at the expense of space for another, and thus they are "rival," because each point crowds out some other point.

Non-linear mediums like Canopy are non-rival: [[Non-linear mediums are non-rival for space]], although [[they are rival for attention]].

Non-linear mediums are non-rival for space: Non-linear media can grow happily to any size, and so no point is taking space from any other point, because every category of point has a its own special location in the project.

They are rival for attention: Because Canopy presents information in a hierarchical format, even if there is unlimited space for points in general, there is limited space for a point to get "top billing" in a high-order paragraph. So in that sense even Canopy isn't entirely "non-rival."

Non-rival mediums have various benefits: When different points don't take space from one another, then authors can adopt an attitude of default inclusion, making space for any point any reader might want to see, even if most readers aren't interested, (thus creating works of greater [[scale]] and [[customization]].)


* Proportionality: Proportionality is the idea that because [[Canopy organizes information hierarchically]], if [[one reads top to bottom]], [[one will end up with a proportional understanding]], [[unlike reading a book or article]].

Canopy organizes information hierarchically: Canopy takes a domain and lists the important topics in it, and for each topic it lists the major ideas, and for the major ideas it lists the details, etc.

One reads top to bottom: If one reads about all the big ideas before burrowing into details.

One will end up with a proportional understanding: Whether a person has one hour to read about a subject or one year, if they read the hierarchy top-down, then they will end up with the most accurate picture of the subject matter possible given that time constraint, because they are always reading about the big ideas before going into any details.

Unlike reading a book or article: When reading a book or article, if one only has time for a small portion of it, it is very hard to allocate that time effectively to find the subset of the work that paints the most accurate picture given the time constraint. One is likely to end up with certain concepts in great detail, while not knowing anything about certain other ones.


* Reusability: Reusability is the idea that instead of [[producing large linear units of explanation]], [[an author can produce reusable blocks of content]], and [[reusable content utilizes the author's time more efficiently]], and [[increases explanation quality]]. But [[how is this different from the internet generally?]]

Producing large linear units of explanation: Generally authors write long books or articles, and readers are reading the entire sequence from beginning to end.

An author can produce reusable blocks of content: If an author produces a reusable explanation of a certain idea, then readers of multiple different explanations are able to access it in the context of their various traversals of the content, as if multiple articles were reusing the same paragraphs, and [[reusable content is similar to spoken explanation]].

Reusable content is similar to spoken explanation: A human explainer will say the same idea or opinion in multiple contexts whenever it comes up, which means their explanation of that idea is "reusable" within their various explanations.

Reusable content utilizes the author's time more efficiently: When an author invests in producing a high-quality explanation of a certain concept, if it is a paragraph in a book, it is only seen by readers of that entire book, whereas with Canopy, many consumers requesting different explanations will see that same content in different contexts, leveraging the author's investment more efficiently.

Increases explanation quality: If the same ideas are written and rewritten as ephemeral content, then they might come to lack quality and nuance, but if an author can reuse a certain explanation many times, they are incentivized to invest in it, and many more people will receive the highest quality explanation of a concept available.

How is this different from the internet generally? The entire internet is composed of articles that make hyperlinked reference to other articles, so how is the reusability of Canopy explanations different than the regular internet? The internet is composed of [[distinct explanations that reference one another]], whereas [[Canopy produces single explanations out of reusable pieces]], which helps [[avoid fragmentation in the mind of the reader]].

Distinct explanations that reference one another: Articles on the internet are distinct explanations with fixed sequence, which happen to reference other such articles. In following a hyperlink, one is changing context and subject matter.

Canopy produces single explanations out of reusable pieces: In Canopy, links refer to elements of the current subject matter, and selecting links displays explanation of those elements, producing one continuous explanation, not a set of distinct, tangentially related explanations, like internet articles.

Avoid fragmentation in the mind of the reader: [[Reading connected paragraphs connects content in the mind]], whereas [[reading disconnected explanations produces disconnected understandings]], and [[what the reader wants is a single merged understanding]]. Canopy thus reduces [[fragmentation]] by enabling [[mergability]].

Reading connected paragraphs connects content in the mind: When a reader reads multiple paragraphs that are organically connected, with some aspect of one paragraph being the subject matter of the next paragraph, the content has a much better chance of becoming connected in their mind, so that they will be able to recall one idea when considering the other.

Reading disconnected explanations produces disconnected understandings: If one explanation sends me by hyperlink to a different location that has a new explanation that isn't explicitly connected to a concrete aspect of the previous one, it will be difficult when thinking about the first to remember that I read the second, and I am likely to develop two siloed understandings of the two articles, with neither bringing to mind the other.

What the reader wants is a single merged understanding: What the reader needs is a composite understanding that merges information from the two explanations, making facts from either one available when they recall the subject matter. This can be done by the reader manually, but it can also be done by the writer in the first place.


[Inbox]

Subtopic paragraphs should be cumulative, and when someone gives cumulative descriptions like in an email thread, it is an approximation of subtopic chains.

Explanations of the same thing which comes to exclude different things, eg Chomskian linguistics to exclude a rival theory versus Chomskian linguistics to exclude magic, are two different explanations, or at least, two parts of an explanation.

A series of progressively detailing images.

To convert text to diagraph you have to ask what is it coming to exclude, ie what is the background that the statement is presuming.

If every link is either a dependency or a composition, what is the bidirectional link connecting subordinate topics to parents and vice versa? In a sense both are offered additional context for the other, ie both can be specifications relative to the other. With A you can have AB, and with B you can have AB.

Maybe everything is specification and clarification is just specification of something that you probably already know.

In the global namespace, also use post-phrasing eg "the codebase: The Canopy Codebase has xyz."

People speak about things like motivation questions in students like it is a methodology problem, not a content problem, when you can get pretty far enumerating the questions literally.

Information like a verb table isn't necessarily stored in memory that way, eg irregular verbs might be stored as subpoints to regular ones.

If all follow-up points are done with links, then additional clauses of a paragraph are more clearly "breath" related, whereas otherwise you are using adjacency for multiple things.

Applications of Canopy in programming: The original author of a codebase is often more comfortable maintaining it than are later team members. Canopy can help original authors organize the information they know, such as which parts of the code power which features, where certain decisions are made, and what the original motivations for various design choices were. One could begin at a business requirement, and look at the features that satisfy it, the design decisions made for that feature, and the code that implements it, or one could begin with a line of code and work up, seeing what it does, and why, all the way to the original business requirement.

- The train of thought leading to each test case, from a description of the system under test.

- Applications of Canopy in academia and journalism: Some of the same subjects that are discussed in popularly are also studied in academic institutions. If explanations that were produced in academia were organized differently, it might be more easy for journalists and popular commentators to connect stories of the day to long-term historical or economic analysis, allowing readers who begin at a story to drill into its larger themes, and for students of theoretical ideas to find practical examples of how they play out in the world.

- Canopy can be used as your school notebook for a class, and your goal in reviewing your notes is to come up with an explanation that subsumes all the points made in class, why was each necessary.

- Applications of Canopy in library science: For any set of books, one can imagine a librarian who has read all of them, and could answer questions like "what author addresses this or that point", and without taking sides, that librarian could through quotations put one author in conversation with another, and could list for you all the arguments made in the literature for one position or the other, and the responses, in neutral terms, exerting judgement only in the potential meaning of terms in ordinary language. Therefore, for any set of static resources and a shared language, there exists a set of valid "library diagraphs", which present the indexing of the different parts of those resources by what questions they address and what answers they give, and it might be desirable to produce such diagraphs for popular consumption.

- Giving information in stages creates earlier images that forereference later ones, so that the information is recallable.

Compare diagraph to programming, eg scope, call stack, functional paradigms versus imperative, etc.

Spaced repetition heat map

A source of fragmentation is that experts use generic predication rather than explicit forereference because for _them_ it _is_ a reliable reference.

The way that you sometimes zoom in and sometimes zoom out to analyze a thing

A course is like a diagraph in that everything you mention has to be connected to some previous thing, subsuming even the whole discipline.

We can add invariants like "how do you know" to every point.

Maybe there is a habit of only referencing entities even in prose if they have been imported via link in that paragraph or a direct ancestor.

Overlapping entities like a perek with sugya subtopics and the sugyas as topics, and the differences in how the same information is covered in those two contexts.

If I make a provisional subcategory I can add a note to the parent reminding that it exists and should receive a forereference at some point  ie AT: how do I get visibility into nearby categories that need to be subsumed?

- Maybe when all the children of a point cohere into 4 subcategories I descend.

- Most disambiguation is probably all global references, and maybe even new categories at every level.

I didn't see subcategories could proceed

Canopy codebase: The Canopy codebase contains the Canopy front-end library, the Canopy parser library, and the Canopy command-line interface.

Canopy front-end library: The Canopy codebase contains a front-end library. The front-end code can be found in the repository. The front-end library consumes JSON produced by the parser library which is served by a Canopy server. The code base implements the Canopy project.

The front-end code can be found in the repository: The front-end code can be found in the repository under the `src/client` path.

Canopy parser library: The Canopy codebase contains the backend code for the parser that Canopy uses to convert diagraph script into the JSON that is consumed by the front-end library.

Canopy command-line interface: Canopy has a command-line interface that you can use to set up a Canopy project, convert dgs files into JSON, and run a Canopy server!

Design principles of Canopy: Canopy is designed to mimic the interface of human explanation. The Canopy parser is intended to recognize the same patterns in text as does a human listener.

Mimic the interface of human explanation: Canopy is intended to present the same options to the reader as does a human explainer. The same way that an expert can summarize a domain and then be asked follow up questions, Canopy presents a small amount of information that contains within it follow-up queries that are supported.

The same patterns in text as does a human listener: When a later paragraph references an earlier one, it shouldn't need to be identified with a hypertext reference, because a human reader wouldn't need hypertext to identify the reference as a reference. It must be that the reader recognizes something in the later paragraph they saw in the earlier one, and on this basis recognizes the reference. Canopy is intended to make the same recognition based on the same information.

- Links that reify ordinary language reference - synchronized with no way for links to change from linguistic reference.

The diagraph data structure: There is a definition of diagraph. There are things that make good diagraph. There are different methods of making diagraph. There are implementations of the diagraph data structure.

Definition of diagraph: Diagraph is a graph data structure. Traversals of diagraph are valid prose explanations. A diagraph is composed of a global namespace of topics. Each topic contains a local namespace of subtopic names. One subtopic of the topic matches the topic.

- For each line of the definition, give an example as a local reference.

Good diagraph: Certain qualities make diagraph good.

Methods of making diagraph: There are different methods for making diagraph. You can use the canopy bulk mode for example.

Canopy bulk mode: The canopy bulk mode is a CLI tool for making diagraph.

- Library diagraph and the motivation ie the need for uncontroversial summaries of what views exist without trust for the summarizer.

Implementations of the diagraph data structure: Of which, the Canopy project is one.


Functions of the Canopy library: There are certain functions the Canopy library performs, like run a Canopy server.

Canopy server: A Canopy server is a server that serves and supports the Canopy.js library. The Canopy server can be a static assets server or a node.js script. The server delivers the Canopy.js library on the first request, and then handles subsequent requests from the browser for JSON data files. The Canopy server is necessary to view an example of the Canopy project.

Diagraph script: Diagraph script is a natural language text format that Canopy uses to construct a Canopy website.

Motivations for Canopy: There are problems Canopy is intended to solve. There are interactions Canopy is designed to support. There are reasons why a solution like Canopy should be possible.

- The idea that experts have a massively redundant graph where every topic relates to every other topic.

Problems Canopy is intended to solve: There are certain inefficiencies in the current creation and consumption of explanation that motivate the creation and use of Canopy.

- Canopy can be compared to other solutions to those same problems, wikis, annotation, semantic web, etc.

Interactions Canopy is designed to support: There are certain desirable ways of interacting with explanation that Canopy supports.

Reasons why a solution like Canopy should be possible: Here they are.

THe fact that infinite tree storage has only existed for a bit, and binary search being the most efficient way to look things up.

How to use Canopy: There are instructions for how to produce diagraph with Canopy, and how to use a Canopy website.

How to use a Canopy website: Here is how.

How to produce diagraph with Canopy: Canopy turns a directory of diagraph script files into -


First, it is useful to understand the [ problems Canopy solves]. Then, we can discuss [how Canopy helps], and also specifically, the [ applications of Canopy] in different use-cases. Those interested can inquire into [how Canopy works] on a technical level and understand the [ design of Canopy]. Those wishing to make their own project with Canopy can read about [how Canopy is used] practically speaking. Anyone unfamiliar with the user interface can do a short [walk through]. (Try using the arrow keys.)

Canopy is a JavaScript library for creating and browsing the diagraph data structure. There are functions of the Canopy library. There are motivations for Canopy. There are design principles of Canopy. There are applications of Canopy. There is a way how to use Canopy. There is a Canopy codebase.

- Canopy has features/desiderata? There need to be clause links, so that you can zoom in on a description white paper archive notes
- English as programming language
- Use question topic names.

When does a person use a subtopic eg "France#Economy" and when does a person use a topic

Different kinds of diagraph eg pedagogic diagraph where a simple version of the idea is the root, and specifications are complications.

Diagraph is a dependency graph


